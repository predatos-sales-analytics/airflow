services:
  postgres:
    container_name: sales-postgres
    image: postgres:16
    environment:
      POSTGRES_USER: sales
      POSTGRES_PASSWORD: sales
      POSTGRES_DB: sales
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 512m
        reservations:
          cpus: '0.5'
          memory: 256m
    volumes:
      - postgres-db-volume:/var/lib/postgresql/data
      - ${PROJ_DIR:-.}/docker/postgres/init-sales-db.sh:/docker-entrypoint-initdb.d/init-sales-db.sh:ro
      - ${PROJ_DIR:-.}/data:/data:ro
    healthcheck:
      test: ['CMD-SHELL', 'pg_isready -U sales']
      interval: 5s
      retries: 5
      start_period: 5s
    ports:
      - '5432:5432'

  spark-master:
    container_name: sales-spark-master
    image: apache/spark:3.5.7
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.master.Master
    environment:
      - SPARK_MASTER_HOST=spark-master
      - SPARK_MASTER_PORT=7077
      - SPARK_MASTER_WEBUI_PORT=8082
      - SPARK_DAEMON_MEMORY=384m
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 1g
        reservations:
          cpus: '0.5'
          memory: 512m
    ports:
      - '7077:7077'
      - '8082:8082'
    volumes:
      - ${PROJ_DIR:-.}/data:/opt/spark/work-dir/data:ro
      - ${PROJ_DIR:-.}/src:/opt/spark/work-dir/src:ro
      - ${PROJ_DIR:-.}/config:/opt/spark/work-dir/config:ro
      - ${PROJ_DIR:-.}/output:/opt/spark/work-dir/output

  spark-worker:
    container_name: sales-spark-worker
    build:
      context: .
      dockerfile: docker/spark-worker/Dockerfile
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    user: root
    environment:
      - SPARK_WORKER_CORES=4
      - SPARK_WORKER_MEMORY=4g
      - SPARK_WORKER_WEBUI_PORT=8081
    deploy:
      resources:
        limits:
          cpus: '4.0'
          memory: 4g
        reservations:
          cpus: '2.0'
          memory: 2g
    depends_on:
      - spark-master
    ports:
      - '8083:8081'
    volumes:
      - ${PROJ_DIR:-.}/data:/opt/spark/work-dir/data:ro
      - ${PROJ_DIR:-.}/src:/opt/spark/work-dir/src:ro
      - ${PROJ_DIR:-.}/config:/opt/spark/work-dir/config:ro
      - ${PROJ_DIR:-.}/output:/opt/spark/work-dir/output

  spark-client:
    container_name: sales-spark-client
    build:
      context: .
      dockerfile: docker/spark-client/Dockerfile
    command: bash -c "mkdir -p /opt/spark/work-dir/output && chmod -R 777 /opt/spark/work-dir/output && sleep infinity"
    user: root
    environment:
      - POSTGRES_HOST=postgres
      - POSTGRES_PORT=5432
      - POSTGRES_DB=sales
      - POSTGRES_USER=sales
      - POSTGRES_PASSWORD=sales
      - SPARK_MASTER_URL=spark://spark-master:7077
      - PYSPARK_PYTHON=/usr/bin/python3
      - PYTHONPATH=/opt/spark/work-dir/src:/opt/spark/work-dir/config
      - DATA_PATH=/opt/spark/work-dir/data
      - OUTPUT_PATH=/opt/spark/work-dir/output
    depends_on:
      - spark-master
      - postgres
    volumes:
      - ${PROJ_DIR:-.}/data:/opt/spark/work-dir/data:ro
      - ${PROJ_DIR:-.}/src:/opt/spark/work-dir/src:ro
      - ${PROJ_DIR:-.}/config:/opt/spark/work-dir/config:ro
      - ${PROJ_DIR:-.}/output:/opt/spark/work-dir/output
      - ${PROJ_DIR:-.}/run_pipeline.py:/opt/spark/work-dir/run_pipeline.py:ro
    working_dir: /opt/spark/work-dir

volumes:
  postgres-db-volume:

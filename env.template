# Apache Airflow - Variables de Entorno
# Copiar este archivo a .env y ajustar los valores según tu entorno

# ============================================================================
# AIRFLOW CORE
# ============================================================================

# ID de usuario para Airflow (Linux/Mac: usar id -u, Windows: 50000)
# Generar con: echo -e "AIRFLOW_UID=$(id -u)" > .env (Linux/Mac)
AIRFLOW_UID=50000

# Clave de encriptación Fernet para Airflow
# Generar con: python -c "from cryptography.fernet import Fernet; print(Fernet.generate_key().decode())"
AIRFLOW_FERNET_KEY=

# ============================================================================
# POSTGRES - Base de Datos de Airflow
# ============================================================================

# Usuario y contraseña para la base de datos de Airflow (metadata)
AIRFLOW_DB_USER=airflow
AIRFLOW_DB_PASSWORD=airflow
AIRFLOW_DB_NAME=airflow

# ============================================================================
# POSTGRES - Base de Datos de Ventas
# ============================================================================

# Host y puerto de Postgres (usar 'postgres' dentro de Docker, 'localhost' desde host)
POSTGRES_HOST=postgres
POSTGRES_PORT=5432

# Base de datos de ventas
POSTGRES_DB=sales
POSTGRES_USER=sales
POSTGRES_PASSWORD=sales

# ============================================================================
# SPARK
# ============================================================================

# URL del Spark Master (usar 'spark://spark-master:7077' dentro de Docker)
SPARK_MASTER_URL=spark://spark-master:7077

# JARs adicionales para Spark (JDBC drivers, etc.)
SPARK_JARS_PACKAGES=org.postgresql:postgresql:42.7.3

# Configuraciones adicionales de Spark (formato: SPARK_CONF__<key>=<value>)
# Ejemplo: SPARK_CONF__spark.executor.memory=4g
# SPARK_CONF__spark.executor.memory=4g
# SPARK_CONF__spark.driver.memory=4g

# ============================================================================
# RUTAS DE DATOS
# ============================================================================

# Ruta base de datos dentro del contenedor (normalmente /opt/airflow/data)
DATA_PATH=/opt/airflow/data

# Ruta de salida dentro del contenedor (normalmente /opt/airflow/output)
OUTPUT_PATH=/opt/airflow/output

# Fuente de datos: 'postgres' o 's3'
# 'postgres': Lee desde base de datos Postgres
# 's3': Lee directamente desde rutas en S3 (usar variables S3_* de abajo)
DATA_SOURCE=postgres

# Configuración S3 para DATA_SOURCE='s3'
# Prefijo base que contiene raw-data/transactions y raw-data/products
# S3_DATA_BASE_URI=s3://exec-summary-frank-bullfrog/raw-data

# Rutas específicas (puedes sobreescribir las tres o dejar que se construyan desde el prefijo)
# S3_TRANSACTIONS_PATH=s3://exec-summary-frank-bullfrog/raw-data/transactions
# S3_CATEGORIES_PATH=s3://exec-summary-frank-bullfrog/raw-data/products/Categories.csv
# S3_PRODUCT_CATEGORIES_PATH=s3://exec-summary-frank-bullfrog/raw-data/products/ProductCategory.csv

# Separador usado por los CSV de S3 (default: | )
# S3_FILE_SEPARATOR=|

# ============================================================================
# PYTHON
# ============================================================================

# Ruta del intérprete Python para PySpark
PYSPARK_PYTHON=/usr/local/bin/python

# ============================================================================
# OPCIONAL - Configuraciones Avanzadas
# ============================================================================

# Tamaño de muestra para transacciones (opcional, dejar vacío para todas)
# SAMPLE_SIZE=100000

# Soporte mínimo para FP-Growth (default: 0.05)
# FP_GROWTH_MIN_SUPPORT=0.05

# Confianza mínima para FP-Growth (default: 0.4)
# FP_GROWTH_MIN_CONFIDENCE=0.4


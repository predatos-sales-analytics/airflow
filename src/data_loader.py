"""
MÃ³dulo para carga de datos desde PostgreSQL.

Este mÃ³dulo carga datos directamente desde la base de datos PostgreSQL
usando JDBC de Spark para procesamiento distribuido.
"""

from pyspark.sql import SparkSession, DataFrame
from pyspark.sql.functions import col, explode, split, trim
import os
from typing import List, Optional


class DataLoader:
    """Clase para cargar datos desde PostgreSQL usando Spark JDBC."""

    def __init__(self, spark: SparkSession):
        """
        Inicializa el cargador de datos.

        Args:
            spark: SesiÃ³n de Spark
        """
        self.spark = spark

        # ConfiguraciÃ³n de PostgreSQL desde variables de entorno
        postgres_host = os.getenv("POSTGRES_HOST", "postgres")
        postgres_port = os.getenv("POSTGRES_PORT", "5432")
        postgres_db = os.getenv("POSTGRES_DB", "sales")
        postgres_user = os.getenv("POSTGRES_USER", "sales")
        postgres_password = os.getenv("POSTGRES_PASSWORD", "sales")

        self.jdbc_url = (
            f"jdbc:postgresql://{postgres_host}:{postgres_port}/{postgres_db}"
        )
        self.jdbc_properties = {
            "user": postgres_user,
            "password": postgres_password,
            "driver": "org.postgresql.Driver",
        }

        print(f"ðŸ“¡ ConfiguraciÃ³n JDBC: {postgres_host}:{postgres_port}/{postgres_db}")

    def load_categories(self) -> DataFrame:
        """
        Carga el catÃ¡logo de categorÃ­as desde PostgreSQL.

        Returns:
            DataFrame con categorÃ­as (category_id: int, category_name: string)
        """
        print("   â†’ Cargando tabla 'categories' desde PostgreSQL...")

        df = self.spark.read.jdbc(
            url=self.jdbc_url, table="categories", properties=self.jdbc_properties
        )

        # Asegurar tipos correctos
        df = df.withColumn("category_id", col("category_id").cast("int"))

        return df

    def load_product_categories(self) -> DataFrame:
        """
        Carga la relaciÃ³n productos-categorÃ­as desde PostgreSQL.

        Returns:
            DataFrame con relaciÃ³n (product_id: int, category_id: int)
        """
        print("   â†’ Cargando tabla 'product_categories' desde PostgreSQL...")

        df = self.spark.read.jdbc(
            url=self.jdbc_url,
            table="product_categories",
            properties=self.jdbc_properties,
        )

        # Asegurar tipos correctos
        df = df.withColumn("product_id", col("product_id").cast("int"))
        df = df.withColumn("category_id", col("category_id").cast("int"))

        return df

    def load_transactions(self, store_ids: Optional[List[str]] = None) -> DataFrame:
        """
        Carga transacciones desde PostgreSQL.

        Args:
            store_ids: Lista de IDs de tiendas a cargar (None = todas)

        Returns:
            DataFrame con transacciones
        """
        print("   â†’ Cargando tabla 'transactions' desde PostgreSQL...")

        if store_ids is None:
            # Cargar todas las transacciones
            df = self.spark.read.jdbc(
                url=self.jdbc_url, table="transactions", properties=self.jdbc_properties
            )
        else:
            # Filtrar por tiendas especÃ­ficas usando pushdown predicate
            store_ids_str = ",".join(str(sid) for sid in store_ids)
            query = f"(SELECT * FROM transactions WHERE store_id IN ({store_ids_str})) as filtered"

            df = self.spark.read.jdbc(
                url=self.jdbc_url, table=query, properties=self.jdbc_properties
            )

        # Asegurar tipos correctos
        df = df.withColumn("store_id", col("store_id").cast("int"))
        df = df.withColumn("customer_id", col("customer_id").cast("int"))

        return df

    def explode_transactions(self, df: DataFrame) -> DataFrame:
        """
        Explota las transacciones para tener un producto por fila.

        La columna 'products' contiene IDs de productos separados por espacio.
        Este mÃ©todo crea una fila por cada producto en cada transacciÃ³n.

        Args:
            df: DataFrame de transacciones

        Returns:
            DataFrame con productos explodidos (una fila por producto)
        """
        print("   â†’ Explodiendo productos (una fila por producto)...")

        # Separar la columna de productos en array
        df_with_array = df.withColumn(
            "product_array", split(trim(col("products")), " ")
        )

        # Explotar el array para tener un producto por fila
        df_exploded = df_with_array.withColumn(
            "product_id", explode(col("product_array"))
        ).select("transaction_date", "store_id", "customer_id", "product_id")

        # Convertir product_id a entero
        df_exploded = df_exploded.withColumn(
            "product_id", col("product_id").cast("int")
        )

        return df_exploded

    def get_available_stores(self) -> List[str]:
        """
        Obtiene la lista de tiendas disponibles en la base de datos.

        Returns:
            Lista de IDs de tiendas
        """
        print("   â†’ Obteniendo lista de tiendas desde PostgreSQL...")

        query = (
            "(SELECT DISTINCT store_id FROM transactions ORDER BY store_id) as stores"
        )

        df = self.spark.read.jdbc(
            url=self.jdbc_url, table=query, properties=self.jdbc_properties
        )

        store_ids = [str(row.store_id) for row in df.collect()]

        print(f"   âœ… {len(store_ids)} tiendas encontradas: {', '.join(store_ids)}")

        return store_ids

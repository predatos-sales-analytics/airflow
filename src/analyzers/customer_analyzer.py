"""
M√≥dulo para an√°lisis y segmentaci√≥n de clientes usando K-Means.

Implementa clustering para identificar grupos de clientes seg√∫n su
comportamiento de compra.
"""

from pyspark.sql import DataFrame, SparkSession
from pyspark.sql.functions import (
    col,
    count,
    sum as spark_sum,
    avg,
    min as spark_min,
    max as spark_max,
    countDistinct,
    size,
    split,
    trim,
    desc,
    to_date,
    lit,
)
from pyspark.ml.feature import VectorAssembler, StandardScaler
from pyspark.ml.clustering import KMeans
from pyspark.ml.evaluation import ClusteringEvaluator
from typing import Dict, Any, Tuple
import os


class CustomerAnalyzer:
    """Clase para realizar an√°lisis y segmentaci√≥n de clientes con K-Means."""

    def __init__(self, spark: SparkSession):
        """
        Inicializa el analizador de clientes.

        Args:
            spark: Sesi√≥n de Spark
        """
        self.spark = spark

    def prepare_customer_features(
        self,
        df_transactions: DataFrame,
        df_transactions_exploded: DataFrame,
        df_product_categories: DataFrame = None,
    ) -> DataFrame:
        """
        Prepara features para clustering de clientes.

        Features requeridas seg√∫n enunciado:
        - Frecuencia: n√∫mero de transacciones
        - N√∫mero de productos distintos: diversidad de productos
        - Volumen total: total de productos comprados
        - Diversidad de categor√≠as: n√∫mero de categor√≠as distintas compradas

        Args:
            df_transactions: DataFrame de transacciones sin explotar
            df_transactions_exploded: DataFrame de transacciones explotadas
            df_product_categories: DataFrame de relaci√≥n producto-categor√≠a (opcional)

        Returns:
            DataFrame con features por cliente
        """
        print("\nüîß Preparando features para segmentaci√≥n de clientes...")
        print("-" * 70)

        # Feature 1: Frecuencia (n√∫mero de transacciones)
        df_frequency = df_transactions.groupBy("customer_id").agg(
            count("*").alias("frequency")
        )

        # Feature 2: N√∫mero de productos distintos (diversidad de productos)
        df_unique_products = df_transactions_exploded.groupBy("customer_id").agg(
            countDistinct("product_id").alias("unique_products")
        )

        # Feature 3: Volumen total (total de productos comprados)
        df_volume = df_transactions.groupBy("customer_id").agg(
            spark_sum(size(split(trim(col("products")), " "))).alias("total_volume")
        )

        # Feature 4: Diversidad de categor√≠as
        if df_product_categories is not None:
            # Join con categor√≠as de productos
            df_exploded_with_cats = df_transactions_exploded.join(
                df_product_categories, "product_id", "left"
            )
            df_unique_categories = df_exploded_with_cats.groupBy("customer_id").agg(
                countDistinct("category_id").alias("unique_categories")
            )
        else:
            # Si no hay categor√≠as, crear columna con 0
            print(
                "   ‚ö†Ô∏è No se proporcion√≥ df_product_categories, usando 0 para diversidad de categor√≠as"
            )
            df_unique_categories = (
                df_transactions.groupBy("customer_id")
                .agg(count("*").alias("temp"))
                .select("customer_id", lit(0).cast("int").alias("unique_categories"))
            )

        # Combinar todas las features
        df_features = (
            df_frequency.join(df_unique_products, "customer_id", "inner")
            .join(df_volume, "customer_id", "inner")
            .join(df_unique_categories, "customer_id", "inner")
        )

        # Convertir customer_id a int
        df_features = df_features.withColumn(
            "customer_id", col("customer_id").cast("int")
        )

        # Mostrar estad√≠sticas
        stats = df_features.select(
            count("*").alias("total_customers"),
            avg("frequency").alias("avg_frequency"),
            avg("total_volume").alias("avg_volume"),
            avg("unique_products").alias("avg_unique_products"),
            avg("unique_categories").alias("avg_unique_categories"),
        ).collect()[0]

        print(f"üìä Total de clientes: {stats['total_customers']:,}")
        print(f"üìä Frecuencia promedio: {stats['avg_frequency']:.2f} transacciones")
        print(f"üìä Volumen total promedio: {stats['avg_volume']:.2f} productos")
        print(f"üìä Productos distintos promedio: {stats['avg_unique_products']:.2f}")
        print(f"üìä Categor√≠as distintas promedio: {stats['avg_unique_categories']:.2f}")

        return df_features

    def perform_kmeans_clustering(
        self, df_features: DataFrame, n_clusters: int = 4
    ) -> Tuple[DataFrame, Any]:
        """
        Ejecuta clustering K-Means en los clientes.

        Args:
            df_features: DataFrame con features de clientes
            n_clusters: N√∫mero de clusters a crear

        Returns:
            Tuple (DataFrame con asignaciones de clusters, modelo K-Means)
        """
        print(f"\nüéØ Ejecutando K-Means con {n_clusters} clusters...")
        print("-" * 70)

        # Seleccionar features num√©ricas para clustering (seg√∫n enunciado)
        feature_cols = [
            "frequency",  # Frecuencia
            "unique_products",  # N√∫mero de productos distintos
            "total_volume",  # Volumen total
            "unique_categories",  # Diversidad de categor√≠as
        ]

        # Ensamblar features en vector
        assembler = VectorAssembler(inputCols=feature_cols, outputCol="features_raw")
        df_assembled = assembler.transform(df_features)

        # Escalar features (importante para K-Means)
        scaler = StandardScaler(
            inputCol="features_raw", outputCol="features", withStd=True, withMean=True
        )
        scaler_model = scaler.fit(df_assembled)
        df_scaled = scaler_model.transform(df_assembled)

        # Entrenar K-Means
        kmeans = KMeans(
            k=n_clusters,
            featuresCol="features",
            predictionCol="cluster",
            seed=42,
            maxIter=20,
        )

        model = kmeans.fit(df_scaled)
        df_clustered = model.transform(df_scaled)

        # Evaluar clustering
        evaluator = ClusteringEvaluator(
            featuresCol="features", predictionCol="cluster", metricName="silhouette"
        )
        silhouette = evaluator.evaluate(df_clustered)

        print(f"‚úÖ Clustering completado")
        print(f"üìä Silhouette Score: {silhouette:.4f}")
        print(f"üìä Centros de clusters: {n_clusters}")

        # Seleccionar columnas relevantes
        df_result = df_clustered.select(
            "customer_id",
            "frequency",
            "unique_products",
            "total_volume",
            "unique_categories",
            "cluster",
        )

        return df_result, model

    def analyze_clusters(
        self, df_clustered: DataFrame, n_clusters: int
    ) -> Dict[str, Any]:
        """
        Analiza y describe los clusters encontrados.

        Args:
            df_clustered: DataFrame con clientes y sus clusters
            n_clusters: N√∫mero de clusters

        Returns:
            Diccionario con descripci√≥n de cada cluster
        """
        print(f"\nüìä Analizando caracter√≠sticas de cada cluster...")
        print("-" * 70)

        cluster_profiles = {}
        total_customers = df_clustered.count()

        for cluster_id in range(n_clusters):
            print(f"\n{'='*70}")
            print(f"üè∑Ô∏è  CLUSTER {cluster_id}")
            print(f"{'='*70}")

            df_cluster = df_clustered.filter(col("cluster") == cluster_id)
            n_customers = df_cluster.count()

            stats = df_cluster.select(
                avg("frequency").alias("avg_frequency"),
                avg("unique_products").alias("avg_unique_products"),
                avg("total_volume").alias("avg_volume"),
                avg("unique_categories").alias("avg_unique_categories"),
                spark_min("frequency").alias("min_frequency"),
                spark_max("frequency").alias("max_frequency"),
                spark_min("total_volume").alias("min_volume"),
                spark_max("total_volume").alias("max_volume"),
            ).collect()[0]

            print(f"üë• N√∫mero de clientes: {n_customers:,}")
            print(f"üìä Frecuencia promedio: {stats['avg_frequency']:.2f} transacciones")
            print(
                f"üìä Productos distintos promedio: {stats['avg_unique_products']:.2f}"
            )
            print(f"üìä Volumen total promedio: {stats['avg_volume']:.2f} productos")
            print(
                f"üìä Categor√≠as distintas promedio: {stats['avg_unique_categories']:.2f}"
            )

            # Clasificar el cluster y generar descripci√≥n detallada
            avg_freq = stats["avg_frequency"]
            avg_vol = stats["avg_volume"]
            avg_prod_dist = stats["avg_unique_products"]
            avg_cat_dist = stats["avg_unique_categories"]

            # Determinar tipo de cluster y generar descripci√≥n
            if avg_freq > 15 and avg_vol > 50 and avg_cat_dist > 10:
                cluster_label = "VIP / Compradores Premium"
                description = (
                    f"Clientes con alta frecuencia de compra ({avg_freq:.1f} transacciones en promedio), "
                    f"alto volumen de productos ({avg_vol:.0f} productos), amplia diversidad de productos "
                    f"({avg_prod_dist:.0f} productos distintos) y exploran m√∫ltiples categor√≠as "
                    f"({avg_cat_dist:.1f} categor√≠as). Son los clientes m√°s valiosos."
                )
                recommendations = [
                    "Programa de fidelizaci√≥n premium con beneficios exclusivos",
                    "Acceso anticipado a nuevos productos y ofertas especiales",
                    "Asesor personalizado o atenci√≥n prioritaria",
                    "Descuentos por volumen y puntos de recompensa",
                ]
            elif avg_freq > 10:
                cluster_label = "Clientes Frecuentes"
                description = (
                    f"Clientes que compran regularmente ({avg_freq:.1f} transacciones) con buen volumen "
                    f"({avg_vol:.0f} productos). Suelen explorar {avg_prod_dist:.0f} productos distintos "
                    f"y {avg_cat_dist:.1f} categor√≠as. Tienen potencial de crecimiento."
                )
                recommendations = [
                    "Cross-selling y up-selling de productos relacionados",
                    "Recomendaciones personalizadas basadas en historial",
                    "Programas de lealtad con recompensas progresivas",
                    "Ofertas en categor√≠as que a√∫n no han explorado",
                ]
            elif avg_prod_dist > 20 or avg_cat_dist > 8:
                cluster_label = "Exploradores / Diversificadores"
                description = (
                    f"Clientes que buscan variedad: {avg_prod_dist:.0f} productos distintos y "
                    f"{avg_cat_dist:.1f} categor√≠as diferentes, aunque con frecuencia moderada "
                    f"({avg_freq:.1f} transacciones). Valoran la diversidad y experimentaci√≥n."
                )
                recommendations = [
                    "Mostrar productos nuevos y tendencias del mercado",
                    "Destacar diversidad de cat√°logo y categor√≠as especiales",
                    "Campa√±as de descubrimiento con muestras o pruebas",
                    "Recomendaciones de productos similares a los que les gustan",
                ]
            elif avg_freq < 5 and avg_vol < 20:
                cluster_label = "Clientes Ocasionales / Nuevos"
                description = (
                    f"Clientes con bajo compromiso: {avg_freq:.1f} transacciones y {avg_vol:.0f} productos. "
                    f"Compran productos limitados ({avg_prod_dist:.0f} distintos) en pocas categor√≠as "
                    f"({avg_cat_dist:.1f}). Pueden estar empezando o ser clientes ocasionales."
                )
                recommendations = [
                    "Campa√±as de bienvenida para nuevos clientes",
                    "Ofertas de reactivaci√≥n con descuentos especiales",
                    "Programas de referencia para atraer amigos",
                    "Comunicaci√≥n educativa sobre beneficios y productos",
                ]
            else:
                cluster_label = "Clientes Regulares"
                description = (
                    f"Clientes con comportamiento promedio: {avg_freq:.1f} transacciones, "
                    f"{avg_vol:.0f} productos, {avg_prod_dist:.0f} productos distintos y "
                    f"{avg_cat_dist:.1f} categor√≠as. Representan la base de clientes."
                )
                recommendations = [
                    "Ofertas regulares para mantener el inter√©s",
                    "Recordatorios de productos frecuentemente comprados",
                    "Programas de puntos o cashback",
                    "Comunicaci√≥n sobre nuevas categor√≠as y productos",
                ]

            print(f"üè∑Ô∏è  Etiqueta: {cluster_label}")
            print(f"üìù Descripci√≥n: {description}")
            print(f"üí° Recomendaciones:")
            for i, rec in enumerate(recommendations, 1):
                print(f"   {i}. {rec}")

            cluster_profiles[f"cluster_{cluster_id}"] = {
                "cluster_id": cluster_id,
                "label": cluster_label,
                "description": description,
                "n_customers": n_customers,
                "percentage": (
                    (n_customers / total_customers) * 100
                    if total_customers > 0
                    else 0.0
                ),
                "metrics": {
                    "avg_frequency": float(avg_freq),
                    "avg_unique_products": float(avg_prod_dist),
                    "avg_total_volume": float(avg_vol),
                    "avg_unique_categories": float(avg_cat_dist),
                    "min_frequency": int(stats["min_frequency"]),
                    "max_frequency": int(stats["max_frequency"]),
                    "min_volume": int(stats["min_volume"]),
                    "max_volume": int(stats["max_volume"]),
                },
                "business_recommendations": recommendations,
            }

        return cluster_profiles

    def generate_customer_segmentation(
        self,
        df_transactions: DataFrame,
        df_transactions_exploded: DataFrame,
        df_product_categories: DataFrame = None,
        n_clusters: int = 4,
    ) -> Dict[str, Any]:
        """
        Genera segmentaci√≥n completa de clientes usando K-Means.

        Args:
            df_transactions: DataFrame de transacciones
            df_transactions_exploded: DataFrame de transacciones explotadas
            df_product_categories: DataFrame de relaci√≥n producto-categor√≠a (opcional)
            n_clusters: N√∫mero de clusters deseados

        Returns:
            Diccionario con resultados de segmentaci√≥n
        """
        print("\n" + "=" * 70)
        print("üë• SEGMENTACI√ìN DE CLIENTES CON K-MEANS")
        print("=" * 70)

        # 1. Preparar features (incluyendo diversidad de categor√≠as)
        df_features = self.prepare_customer_features(
            df_transactions, df_transactions_exploded, df_product_categories
        )

        # 2. Ejecutar clustering
        df_clustered, model = self.perform_kmeans_clustering(df_features, n_clusters)

        # 3. Analizar clusters
        cluster_profiles = self.analyze_clusters(df_clustered, n_clusters)

        # 4. Mostrar distribuci√≥n de clusters
        print(f"\nüìä Distribuci√≥n de clientes por cluster:")
        df_clustered.groupBy("cluster").agg(count("*").alias("n_customers")).orderBy(
            "cluster"
        ).show()

        print("=" * 70)
        print("‚úÖ Segmentaci√≥n completada exitosamente")

        return {
            "clusters_df": df_clustered,
            "cluster_summary": cluster_profiles,
            "n_clusters": n_clusters,
            "model": model,
        }

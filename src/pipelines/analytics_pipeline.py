"""
Pipeline 2: An√°lisis Anal√≠tico (Semanal)

Genera visualizaciones anal√≠ticas:
- Series temporales (diarias, semanales, mensuales)
- Distribuciones de ventas
- An√°lisis de tendencias
- Correlaciones entre variables

Output: output/analytics/*.json
"""

import sys
import os

sys.path.insert(0, os.path.join(os.path.dirname(__file__), "..", ".."))
sys.path.insert(0, os.path.join(os.path.dirname(__file__), ".."))

from src.config.spark_config import create_spark_session, stop_spark_session
from src.data_loader import DataLoader
from src.analyzers.temporal_analyzer import TemporalAnalyzer
from src.analyzers.statistical_analyzer import StatisticalAnalyzer
from src.json_exporter import JSONExporter
from pyspark.sql.functions import col, concat_ws


class AnalyticsPipeline:
    """Pipeline para generar an√°lisis anal√≠tico."""

    def __init__(self):
        """Inicializa el pipeline."""
        self.spark = None
        self.data_loader = None
        self.temporal_analyzer = None
        self.statistical_analyzer = None
        self.exporter = None

    def run(self):
        """Ejecuta el pipeline completo."""
        try:
            print("=" * 70)
            print("üöÄ PIPELINE 2: AN√ÅLISIS ANAL√çTICO")
            print("=" * 70)

            # Inicializar Spark
            print("\nüì° Inicializando Spark...")
            self.spark = create_spark_session("AnalyticsPipeline")

            # Inicializar componentes
            self.data_loader = DataLoader(self.spark)
            self.temporal_analyzer = TemporalAnalyzer(self.spark)
            self.statistical_analyzer = StatisticalAnalyzer(self.spark)
            self.exporter = JSONExporter()

            # Cargar datos
            print("\nüìÇ Cargando datos desde PostgreSQL...")

            df_transactions = self.data_loader.load_transactions()
            df_product_categories = self.data_loader.load_product_categories()
            df_categories = self.data_loader.load_categories()

            # ============================================================
            # VISUALIZACI√ìN 1: SERIE DE TIEMPO
            # ============================================================
            print("\n" + "=" * 70)
            print("üìà VISUALIZACI√ìN 1: SERIE DE TIEMPO")
            print("=" * 70)

            # An√°lisis temporal diario
            print("\nüìÖ Generando serie temporal diaria...")
            df_daily = self.temporal_analyzer.analyze_daily_sales(
                df_transactions, date_column="transaction_date"
            )
            self.exporter.export_time_series(
                df_daily,
                series_name="daily_sales",
                date_col="date",
                value_col="num_transacciones",
            )

            # An√°lisis temporal semanal
            print("\nüìÖ Generando serie temporal semanal...")
            df_weekly = self.temporal_analyzer.analyze_weekly_sales(
                df_transactions, date_column="transaction_date"
            )
            self.exporter.export_time_series(
                df_weekly,
                series_name="weekly_sales",
                date_col="inicio_semana",
                value_col="num_transacciones",
            )

            # An√°lisis temporal mensual
            print("\nüìÖ Generando serie temporal mensual...")
            df_monthly = self.temporal_analyzer.analyze_monthly_sales(
                df_transactions, date_column="transaction_date"
            )
            # Crear columna de fecha para exportaci√≥n (a√±o-mes)
            from pyspark.sql.functions import concat_ws, lit

            df_monthly_export = df_monthly.withColumn(
                "date", concat_ws("-", col("year"), col("month"))
            )
            self.exporter.export_time_series(
                df_monthly_export,
                series_name="monthly_sales",
                date_col="date",
                value_col="num_transacciones",
            )

            # An√°lisis por d√≠a de la semana
            print("\nüìÖ Generando an√°lisis por d√≠a de la semana...")
            df_day_of_week = self.temporal_analyzer.analyze_day_of_week_patterns(
                df_transactions, date_column="transaction_date"
            )
            # Exportar como distribuci√≥n (no serie temporal)
            self.exporter.export_distribution(
                df_day_of_week,
                distribution_name="day_of_week_patterns",
            )

            # ============================================================
            # VISUALIZACI√ìN 2: BOXPLOT - TOTAL PRODUCTS SOLD POR CATEGOR√çA
            # ============================================================
            print("\n" + "=" * 70)
            print("üì¶ VISUALIZACI√ìN 2: BOXPLOT - TOTAL PRODUCTS SOLD POR CATEGOR√çA")
            print("=" * 70)

            df_category_store_boxplot = (
                self.statistical_analyzer.analyze_category_products_by_store(
                    df_transactions, df_product_categories, df_categories
                )
            )
            self.exporter.export_distribution(
                df_category_store_boxplot,
                distribution_name="category_products_by_store",
            )

            # ============================================================
            # VISUALIZACI√ìN 3: HEATMAP - MATRIZ DE CORRELACI√ìN
            # ============================================================
            print("\n" + "=" * 70)
            print("üî• VISUALIZACI√ìN 3: HEATMAP - MATRIZ DE CORRELACI√ìN")
            print("=" * 70)

            correlation_data = self.statistical_analyzer.calculate_correlation_matrix(
                df_transactions, df_product_categories
            )
            self.exporter.export_correlation_matrix(
                correlation_data, matrix_name="variable_correlation"
            )

            # Metadata de ejecuci√≥n
            self.exporter.export_execution_metadata(
                dag_id="analytics_pipeline",
                task_id="generate_analytics",
                execution_info={
                    "status": "success",
                    "files_generated": [
                        "daily_sales.json",
                        "weekly_sales.json",
                        "monthly_sales.json",
                        "day_of_week_patterns_distribution.json",
                        "category_products_by_store_distribution.json",
                        "variable_correlation.json",
                    ],
                    "visualizations": [
                        "Serie de tiempo (diaria, semanal, mensual)",
                        "Patrones por d√≠a de la semana",
                        "Boxplot - Total productos vendidos por categor√≠a (4 tiendas por categor√≠a)",
                        "Heatmap - Matriz de correlaci√≥n",
                    ],
                },
            )

            print("\n" + "=" * 70)
            print("‚úÖ AN√ÅLISIS ANAL√çTICO GENERADO EXITOSAMENTE")
            print("=" * 70)

        except Exception as e:
            print(f"\n‚ùå Error en An√°lisis Anal√≠tico: {str(e)}")
            import traceback

            traceback.print_exc()
            raise
        finally:
            if self.spark:
                stop_spark_session(self.spark)


if __name__ == "__main__":
    pipeline = AnalyticsPipeline()
    pipeline.run()

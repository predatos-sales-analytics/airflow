"""
Pipeline 1: Resumen Ejecutivo (Diario)

Genera m√©tricas clave para el dashboard:
- Total de ventas (unidades)
- N√∫mero de transacciones
- Top 10 productos
- Top 10 clientes
- D√≠as pico de compra
- Categor√≠as m√°s rentables

Output: output/summary/*.json
"""

import sys
import os

sys.path.insert(0, os.path.join(os.path.dirname(__file__), "..", ".."))
sys.path.insert(0, os.path.join(os.path.dirname(__file__), ".."))

from src.config.spark_config import create_spark_session, stop_spark_session
from src.data_loader import DataLoader
from src.analyzers.summary_metrics import SummaryMetrics
from src.json_exporter import JSONExporter


class ExecutiveSummaryPipeline:
    """Pipeline para generar el resumen ejecutivo."""

    def __init__(self):
        """Inicializa el pipeline."""
        self.spark = None
        self.data_loader = None
        self.metrics_calculator = None
        self.exporter = None

    def run(self):
        """Ejecuta el pipeline completo."""
        try:
            print("=" * 70)
            print("üöÄ PIPELINE 1: RESUMEN EJECUTIVO")
            print("=" * 70)

            # Inicializar Spark
            print("\nüì° Inicializando Spark...")
            self.spark = create_spark_session("ExecutiveSummaryPipeline")

            # Inicializar componentes
            self.data_loader = DataLoader(self.spark)
            self.metrics_calculator = SummaryMetrics(self.spark)
            self.exporter = JSONExporter()

            # Cargar datos
            print("\nüìÇ Cargando datos desde PostgreSQL...")
            print("   [1/5] Cargando transacciones...")
            df_transactions = self.data_loader.load_transactions()
            print(f"   ‚úÖ Transacciones cargadas")

            print("   [2/5] Explodiendo transacciones...")
            df_transactions_exploded = self.data_loader.explode_transactions(
                df_transactions
            )
            print("   ‚úÖ Transacciones explodidas")
            df_transactions_exploded.cache()

            print("   [3/5] Cargando productos-categor√≠as...")
            df_product_categories = self.data_loader.load_product_categories()
            print("   ‚úÖ Productos-categor√≠as cargadas")

            print("   [4/5] Cargando categor√≠as...")
            df_categories = self.data_loader.load_categories()
            print("   ‚úÖ Categor√≠as cargadas")

            print("   [5/5] Todos los datos cargados correctamente")

            # Generar resumen ejecutivo
            results = self.metrics_calculator.generate_executive_summary(
                df_transactions,
                df_transactions_exploded,
                df_product_categories,
                df_categories,
            )

            # Exportar resultados a JSON
            print("\nüíæ Exportando resultados a JSON...")

            # M√©tricas b√°sicas
            self.exporter.export_summary_metrics(
                results["basic_metrics"], filename="basic_metrics.json"
            )

            # Top 10 productos
            self.exporter.export_top_items(
                results["top_products"], item_type="products", top_n=10
            )

            # Top 10 clientes
            self.exporter.export_top_items(
                results["top_customers"], item_type="customers", top_n=10
            )

            # D√≠as pico
            self.exporter.export_top_items(
                results["peak_days"], item_type="peak_days", top_n=10
            )

            # Top categor√≠as
            self.exporter.export_top_items(
                results["top_categories"], item_type="categories", top_n=10
            )

            # Metadata de ejecuci√≥n
            self.exporter.export_execution_metadata(
                dag_id="executive_summary_pipeline",
                task_id="generate_executive_summary",
                execution_info={
                    "status": "success",
                    "files_generated": [
                        "basic_metrics.json",
                        "top_10_products.json",
                        "top_10_customers.json",
                        "top_10_peak_days.json",
                        "top_10_categories.json",
                    ],
                },
            )

            # Liberar cach√©
            df_transactions_exploded.unpersist()

            print("\n" + "=" * 70)
            print("‚úÖ RESUMEN EJECUTIVO GENERADO EXITOSAMENTE")
            print("=" * 70)

        except Exception as e:
            print(f"\n‚ùå Error en Resumen Ejecutivo: {str(e)}")
            import traceback

            traceback.print_exc()
            raise
        finally:
            if self.spark:
                stop_spark_session(self.spark)


if __name__ == "__main__":
    pipeline = ExecutiveSummaryPipeline()
    pipeline.run()

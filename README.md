# ðŸš€ Pipeline de AnÃ¡lisis de Ventas con Prefect

Sistema de anÃ¡lisis de ventas distribuido que utiliza **Prefect** para orquestar pipelines de procesamiento con Apache Spark, generando mÃ©tricas ejecutivas, anÃ¡lisis temporal, segmentaciÃ³n de clientes y recomendaciones de productos.

> **Nota**: Este proyecto ha sido migrado de Apache Airflow a Prefect para mejorar la flexibilidad, observabilidad y facilidad de uso. Ver [PREFECT_GUIDE.md](PREFECT_GUIDE.md) para mÃ¡s detalles.

## ðŸ“‹ Tabla de Contenidos

- [Requisitos](#requisitos)
- [Inicio RÃ¡pido con Prefect](#inicio-rÃ¡pido-con-prefect)
- [Pipelines Disponibles](#pipelines-disponibles)
- [Monitor de Nuevos Datos](#monitor-de-nuevos-datos)
- [Estructura del Proyecto](#estructura-del-proyecto)
- [ConfiguraciÃ³n Avanzada](#configuraciÃ³n-avanzada)
- [GuÃ­a de Prefect](#guÃ­a-de-prefect)
- [Troubleshooting](#troubleshooting)

## ðŸ”§ Requisitos

- **Docker** y **Docker Compose** instalados
- **Git** (para clonar el repositorio)
- Al menos **8 GB de RAM** disponibles para los contenedores
- **Puerto 4200** libre para Prefect UI
- **Puerto 5432** libre para PostgreSQL

## âš¡ Inicio RÃ¡pido con Prefect

### Paso 1: Levantar los servicios

Desde el directorio `airflow/`:

```bash
docker compose up -d
```

Esto iniciarÃ¡:

- **PostgreSQL** (base de datos)
- **Prefect Server** (orquestador - UI en puerto 4200)
- **Prefect Worker** (ejecutor de flows)
- **Spark Master y Worker** (procesamiento distribuido)

**Tiempo estimado**: 2-3 minutos

Verifica que todos los servicios estÃ©n corriendo:

```bash
docker compose ps
```

### Paso 1.5: Configurar Prefect (primera vez)

DespuÃ©s de levantar los servicios por primera vez, configura Prefect:

#### En Linux/Mac:

```bash
./scripts/linux/setup_prefect.sh
```

#### En Windows:

```cmd
scripts\windows\setup_prefect.bat
```

Esto harÃ¡:

- âœ… Crear el work pool necesario
- âœ… Configurar el **monitor de datos automÃ¡tico** (se ejecuta cada 2 minutos)
- âœ… Dejar todo listo para ejecutar flows

Accede a la UI de Prefect en: **http://localhost:4200**

### Paso 2: Cargar los datos

Ahora puedes cargar los datos usando el **flow de Prefect** (automatizado) o los scripts tradicionales.

#### OpciÃ³n A: Usando Prefect Flow (Recomendado)

##### En Linux/Mac:

```bash
./scripts/linux/run_prefect_flow.sh data_loading
```

##### En Windows:

```cmd
scripts\windows\run_prefect_flow.bat data_loading
```

#### OpciÃ³n B: Scripts tradicionales

##### En Linux/Mac:

```bash
./scripts/linux/load_data.sh
```

##### En Windows:

```cmd
scripts\windows\load_data.bat
```

**Nota**: Los archivos CSV deben estar en el directorio `data/` con la estructura:

- `data/products/Categories.csv`
- `data/products/ProductCategory.csv`
- `data/transactions/*.csv`

**Tiempo estimado**: 5-10 minutos (depende del tamaÃ±o de los datos)

### Paso 3: Ejecutar los pipelines

Una vez cargados los datos, ejecuta los pipelines de anÃ¡lisis usando Prefect.

#### Ejecutar todos los pipelines (Flow Maestro)

Este flow ejecuta todos los pipelines en secuencia y sincroniza los resultados al frontend automÃ¡ticamente.

##### En Linux/Mac:

```bash
./scripts/linux/run_prefect_flow.sh master
```

##### En Windows:

```cmd
scripts\windows\run_prefect_flow.bat master
```

#### Ejecutar pipelines individuales

##### En Linux/Mac:

```bash
./scripts/linux/run_prefect_flow.sh executive_summary
./scripts/linux/run_prefect_flow.sh analytics
./scripts/linux/run_prefect_flow.sh clustering
./scripts/linux/run_prefect_flow.sh recommendations
./scripts/linux/run_prefect_flow.sh output_sync  # Sincronizar al frontend
```

##### En Windows:

```cmd
scripts\windows\run_prefect_flow.bat executive_summary
scripts\windows\run_prefect_flow.bat analytics
scripts\windows\run_prefect_flow.bat clustering
scripts\windows\run_prefect_flow.bat recommendations
scripts\windows\run_prefect_flow.bat output_sync
```

**Tiempo estimado**:

- Resumen ejecutivo: 3-5 minutos
- AnalÃ­tica temporal: 5-8 minutos
- Clustering: 8-12 minutos
- Recomendaciones: 10-15 minutos
- Flow maestro completo: 30-45 minutos

**Monitoreo**: Accede a http://localhost:4200 para ver el progreso en tiempo real

### Paso 4: Visualizar resultados

Los resultados se generan en formato JSON en el directorio `output/` y se sincronizan automÃ¡ticamente al frontend si ejecutaste el `master_flow` o el `output_sync_flow`.

```
output/
â”œâ”€â”€ summary/              # MÃ©tricas ejecutivas
â”‚   â”œâ”€â”€ basic_metrics.json
â”‚   â”œâ”€â”€ top_10_products.json
â”‚   â””â”€â”€ top_10_customers.json
â”œâ”€â”€ analytics/            # Series temporales y correlaciones
â”‚   â”œâ”€â”€ daily_sales.json
â”‚   â””â”€â”€ variable_correlation.json
â”œâ”€â”€ advanced/
â”‚   â””â”€â”€ clustering/       # SegmentaciÃ³n de clientes
â”‚       â”œâ”€â”€ cluster_summary.json
â”‚       â””â”€â”€ clustering_visualization.json
â””â”€â”€ recommendations/      # Recomendaciones de productos
    â”œâ”€â”€ product_recs.json
    â””â”€â”€ customer_recs.json
```

**SincronizaciÃ³n automÃ¡tica**: Si ejecutaste el flow maestro o el `output_sync_flow`, los archivos ya estÃ¡n en `../sales-frontend/public/data/`

**Para visualizar**: Ejecuta el dashboard React desde el directorio `sales-frontend/`

## ðŸ“Š Pipelines Disponibles

### 1. Resumen Ejecutivo (`executive_summary`)

Genera mÃ©tricas clave del negocio:

- Total de transacciones y productos vendidos
- Top 10 productos mÃ¡s vendidos
- Top 10 clientes mÃ¡s activos
- Top 10 categorÃ­as por volumen
- DÃ­as pico de ventas

**Salida**: `output/summary/*.json`

### 2. AnalÃ­tica Temporal (`analytics`)

AnÃ¡lisis de patrones temporales y correlaciones:

- Series de tiempo (diarias, semanales, mensuales)
- Patrones por dÃ­a de la semana
- DistribuciÃ³n de productos por categorÃ­a y tienda (boxplot)
- Matriz de correlaciÃ³n entre variables

**Salida**: `output/analytics/*.json`

### 3. SegmentaciÃ³n de Clientes (`clustering`)

Clustering K-Means para identificar perfiles de clientes:

- 4 clusters: VIP/Premium, Exploradores, Ocasionales, Nuevos
- MÃ©tricas por cluster (frecuencia, volumen, diversidad)
- Recomendaciones de negocio por segmento
- VisualizaciÃ³n de clasificaciÃ³n (scatter plot)

**ParÃ¡metros**:

- `--n-clusters`: NÃºmero de clusters (default: 4)

**Salida**: `output/advanced/clustering/*.json`

### 4. Recomendaciones (`recommendations`)

Sistema de recomendaciones basado en reglas de asociaciÃ³n (FP-Growth):

- **Por producto**: Productos complementarios que suelen comprarse juntos
- **Por cliente**: Sugerencias personalizadas segÃºn historial de compra

**ParÃ¡metros**:

- `--min-support`: Soporte mÃ­nimo para reglas (default: 0.005)
- `--min-confidence`: Confianza mÃ­nima para reglas (default: 0.2)

**Salida**: `output/recommendations/*.json`

## ðŸ” Monitor de Nuevos Datos

El sistema incluye un **monitor de datos automÃ¡tico** que detecta cuando hay nuevos datos en la base de datos PostgreSQL.

### âš¡ ConfiguraciÃ³n automÃ¡tica

**El monitor se activa automÃ¡ticamente** al ejecutar `setup_prefect`. No necesitas configuraciÃ³n adicional:

- âœ… Se ejecuta **cada 2 minutos** automÃ¡ticamente
- âœ… Detecta nuevas transacciones, productos o categorÃ­as
- âœ… Notifica cuando hay cambios
- âœ… Guarda el estado entre verificaciones

### CaracterÃ­sticas

- Verificar si hay nuevas transacciones, productos o categorÃ­as
- Comparar el estado actual con la Ãºltima verificaciÃ³n
- Notificar cuando se detectan cambios
- GestiÃ³n completa desde la UI de Prefect

### Â¿CÃ³mo funciona?

El monitor:

1. Consulta el estado actual de la base de datos (conteo de registros y fechas)
2. Compara con el Ãºltimo estado guardado en `output/metadata/data_monitor_state.json`
3. Detecta cambios y notifica
4. Guarda el nuevo estado para la prÃ³xima verificaciÃ³n

### Ejecutar el monitor manualmente

#### En Windows:

```cmd
scripts\windows\monitor_data.bat
```

#### En Linux/Mac:

```bash
./scripts/linux/monitor_data.sh
```

TambiÃ©n puedes usar el script genÃ©rico de flows:

```bash
# Windows
scripts\windows\run_prefect_flow.bat data_monitor

# Linux/Mac
./scripts/linux/run_prefect_flow.sh data_monitor
```

### Ejemplo de salida

Cuando hay datos nuevos:

```
ðŸ”” NUEVOS DATOS DETECTADOS EN LA BASE DE DATOS
======================================================================
ðŸ“ˆ Nuevas transacciones: +1500 (Total: 50000)
ðŸ’¡ Sugerencia: Ejecuta el master flow para actualizar los anÃ¡lisis
   Comando: ./scripts/[windows|linux]/run_prefect_flow.[bat|sh] master
======================================================================
```

Cuando no hay cambios:

```
âœ… Base de datos sin cambios desde la Ãºltima verificaciÃ³n
```

### ConfiguraciÃ³n

El archivo `data_monitor_config.json` contiene la configuraciÃ³n del monitor:

```json
{
  "monitor": {
    "auto_trigger_master": false, // Disparar master flow automÃ¡ticamente
    "save_state": true, // Guardar estado para prÃ³xima verificaciÃ³n
    "check_interval_minutes": 30 // Intervalo recomendado (para deployments)
  }
}
```

### Monitoreo automÃ¡tico (Deployment)

**Â¡El monitor se configura automÃ¡ticamente!** Al ejecutar el script `setup_prefect`, se crea un deployment que ejecuta el monitor **cada 2 minutos** de manera automÃ¡tica.

#### Gestionar el deployment

Desde la UI de Prefect (http://localhost:4200/deployments):

- **Ver estado**: Navega a "Deployments" â†’ "data-monitor-scheduled"
- **Pausar**: Click en "Pause" para detener la ejecuciÃ³n automÃ¡tica
- **Reanudar**: Click en "Resume" para reactivar el monitor
- **Ver historial**: Revisa todas las ejecuciones y sus resultados

El deployment estÃ¡ configurado con:

- â±ï¸ **Intervalo**: Cada 2 minutos
- ðŸ”” **Notificaciones**: Solo cuando hay datos nuevos
- ðŸ’¾ **Estado**: Se guarda automÃ¡ticamente entre ejecuciones
- ðŸš€ **Auto-trigger master**: Deshabilitado (solo notifica)

#### EjecuciÃ³n manual adicional

Si necesitas verificar manualmente sin esperar al prÃ³ximo intervalo:

```bash
# Windows
scripts\windows\monitor_data.bat

# Linux/Mac
./scripts/linux/monitor_data.sh
```

### Monitoreo continuo alternativo (opcional)

Si prefieres usar herramientas del sistema operativo en lugar del deployment de Prefect:

**OpciÃ³n A: Usar un cron job (Linux/Mac)**

```bash
# Verificar cada 30 minutos
*/30 * * * * cd /ruta/al/proyecto/airflow && ./scripts/linux/monitor_data.sh >> logs/monitor.log 2>&1
```

**OpciÃ³n B: Usar Task Scheduler (Windows)**

1. Abre el Programador de tareas
2. Crea una nueva tarea bÃ¡sica
3. Configura para ejecutar `scripts\windows\monitor_data.bat` cada 30 minutos

**Nota**: Si usas estas opciones, considera pausar el deployment de Prefect para evitar ejecuciones duplicadas.

### Estado del monitor

El archivo de estado se guarda en:

```
output/metadata/data_monitor_state.json
```

Contiene:

- NÃºmero total de transacciones
- Fecha de la Ãºltima transacciÃ³n
- Conteos de categorÃ­as y productos
- Timestamp de la Ãºltima verificaciÃ³n

Este archivo se actualiza automÃ¡ticamente despuÃ©s de cada ejecuciÃ³n del monitor.

## ðŸ“ Estructura del Proyecto

```
airflow/
â”œâ”€â”€ docker-compose.yml           # OrquestaciÃ³n de servicios (incluye Prefect)
â”œâ”€â”€ requirements.txt             # Dependencias Python (incluye Prefect)
â”œâ”€â”€ env.template                 # Template de variables de entorno
â”œâ”€â”€ PREFECT_GUIDE.md             # GuÃ­a detallada de Prefect
â”œâ”€â”€ DATA_MONITOR_GUIDE.md        # ðŸ†• GuÃ­a del monitor de nuevos datos
â”‚
â”œâ”€â”€ src/                         # CÃ³digo fuente
â”‚   â”œâ”€â”€ run_pipeline.py          # CLI para ejecutar pipelines (legacy)
â”‚   â”œâ”€â”€ prefect_config.py        # ConfiguraciÃ³n de Prefect
â”‚   â”‚
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â””â”€â”€ spark_config.py      # ConfiguraciÃ³n de Spark
â”‚   â”‚
â”‚   â”œâ”€â”€ deployments/             # ðŸ†• Deployments de Prefect
â”‚   â”‚   â””â”€â”€ monitor_deployment.py          # Deployment automÃ¡tico del monitor
â”‚   â”‚
â”‚   â”œâ”€â”€ flows/                   # ðŸ†• Flows de Prefect
â”‚   â”‚   â”œâ”€â”€ data_loading_flow.py           # Flow de carga de datos
â”‚   â”‚   â”œâ”€â”€ data_monitor_flow.py           # Flow de monitoreo de nuevos datos
â”‚   â”‚   â”œâ”€â”€ executive_summary_flow.py      # Flow resumen ejecutivo
â”‚   â”‚   â”œâ”€â”€ analytics_flow.py              # Flow anÃ¡lisis temporal
â”‚   â”‚   â”œâ”€â”€ clustering_flow.py             # Flow clustering
â”‚   â”‚   â”œâ”€â”€ recommendations_flow.py        # Flow recomendaciones
â”‚   â”‚   â”œâ”€â”€ output_sync_flow.py            # Flow sincronizaciÃ³n frontend
â”‚   â”‚   â”œâ”€â”€ master_flow.py                 # Flow maestro (orquesta todo)
â”‚   â”‚   â””â”€â”€ notifications.py               # Sistema de notificaciones
â”‚   â”‚
â”‚   â”œâ”€â”€ pipelines/               # Pipelines principales (lÃ³gica de negocio)
â”‚   â”‚   â”œâ”€â”€ executive_summary_pipeline.py
â”‚   â”‚   â”œâ”€â”€ analytics_pipeline.py
â”‚   â”‚   â”œâ”€â”€ clustering_pipeline.py
â”‚   â”‚   â””â”€â”€ recommendations_pipeline.py
â”‚   â”‚
â”‚   â”œâ”€â”€ analyzers/               # MÃ³dulos de anÃ¡lisis
â”‚   â”‚   â”œâ”€â”€ summary_metrics.py
â”‚   â”‚   â”œâ”€â”€ temporal_analyzer.py
â”‚   â”‚   â”œâ”€â”€ customer_analyzer.py
â”‚   â”‚   â””â”€â”€ product_analyzer.py
â”‚   â”‚
â”‚   â”œâ”€â”€ data_loader.py           # Carga de datos desde PostgreSQL
â”‚   â””â”€â”€ json_exporter.py         # ExportaciÃ³n de resultados
â”‚
â”œâ”€â”€ scripts/                     # Scripts de utilidad
â”‚   â”œâ”€â”€ linux/
â”‚   â”‚   â”œâ”€â”€ load_data.sh         # Carga de datos (Linux/Mac)
â”‚   â”‚   â”œâ”€â”€ setup_prefect.sh     # ðŸ†• Configurar Prefect
â”‚   â”‚   â”œâ”€â”€ run_prefect_flow.sh  # ðŸ†• Ejecutar flows de Prefect
â”‚   â”‚   â””â”€â”€ monitor_data.sh      # ðŸ†• Monitorear nuevos datos
â”‚   â””â”€â”€ windows/
â”‚       â”œâ”€â”€ load_data.bat        # Carga de datos (Windows)
â”‚       â”œâ”€â”€ setup_prefect.bat    # ðŸ†• Configurar Prefect
â”‚       â”œâ”€â”€ run_prefect_flow.bat # ðŸ†• Ejecutar flows de Prefect
â”‚       â””â”€â”€ monitor_data.bat     # ðŸ†• Monitorear nuevos datos
â”‚
â”œâ”€â”€ data/                        # Datos CSV (montar aquÃ­)
â”‚   â”œâ”€â”€ Categories.csv
â”‚   â”œâ”€â”€ Product_Categories.csv
â”‚   â””â”€â”€ transactions/
â”‚
â”œâ”€â”€ output/                      # Resultados generados
â”‚   â”œâ”€â”€ summary/
â”‚   â”œâ”€â”€ analytics/
â”‚   â”œâ”€â”€ advanced/
â”‚   â”œâ”€â”€ recommendations/
â”‚   â””â”€â”€ metadata/                # ðŸ†• Estado del monitor y metadata
â”‚       â””â”€â”€ data_monitor_state.json
â”‚
â”œâ”€â”€ docker/                      # ConfiguraciÃ³n Docker
â”‚   â”œâ”€â”€ prefect-worker/Dockerfile  # ðŸ†• Dockerfile para Prefect Worker
â”‚   â”œâ”€â”€ spark-client/Dockerfile
â”‚   â”œâ”€â”€ spark-worker/Dockerfile
â”‚   â””â”€â”€ postgres/init-sales-db.sh
â”‚
â”œâ”€â”€ data_monitor_config.json     # ðŸ†• ConfiguraciÃ³n del monitor de datos
â”‚
â””â”€â”€ logs/                        # Logs y ejecuciones
    â””â”€â”€ prefect_runs/            # ðŸ†• Logs de flows de Prefect
```

## âš™ï¸ ConfiguraciÃ³n Avanzada

### Variables de Entorno

Crea un archivo `.env` basado en `env.template`:

```bash
cp env.template .env
```

**Variables principales**:

```bash
# PostgreSQL
POSTGRES_USER=sales
POSTGRES_PASSWORD=sales
POSTGRES_DB=sales

# Prefect
PREFECT_API_URL=http://prefect-server:4200/api

# Spark
SPARK_MASTER_URL=spark://spark-master:7077

# Paths
DATA_PATH=/opt/prefect/work-dir/data
OUTPUT_PATH=/opt/prefect/work-dir/output
FRONTEND_PATH=/opt/prefect/work-dir/frontend
```

### Acceso a Interfaces Web

Una vez levantados los servicios:

- **Prefect UI**: http://localhost:4200
  - Dashboard de flows, ejecuciones, logs y monitoreo
- **Spark Master UI**: http://localhost:8082
  - Monitoreo de jobs de Spark

### ConfiguraciÃ³n de Spark

Edita `src/config/spark_config.py` para ajustar:

- Memoria del driver y executors
- NÃºmero de cores
- Particiones de shuffle

### ParÃ¡metros de Pipelines con Prefect

Los flows de Prefect aceptan parÃ¡metros. Aunque actualmente se ejecutan con valores por defecto desde los scripts, puedes modificarlos en el cÃ³digo de los flows o crear deployments personalizados.

**Valores por defecto:**

- Clustering: `n_clusters=4`
- Recomendaciones: `min_support=0.005`, `min_confidence=0.2`
- Master Flow: ejecuta todos con los valores por defecto y sincroniza al frontend

Ver [PREFECT_GUIDE.md](PREFECT_GUIDE.md) para personalizaciÃ³n avanzada.

## ðŸ› ï¸ Comandos Ãštiles

### GestiÃ³n de servicios

```bash
# Iniciar servicios
docker compose up -d

# Detener servicios
docker compose down

# Ver logs en tiempo real (todos los servicios)
docker compose logs -f

# Ver logs de Prefect
docker compose logs -f prefect-server prefect-worker

# Reiniciar un servicio especÃ­fico
docker compose restart prefect-worker

# Limpiar todo (incluyendo volÃºmenes)
docker compose down -v
```

### Acceso a contenedores

```bash
# Acceder a shell de Prefect Worker
docker compose exec prefect-worker bash

# Acceder a PostgreSQL
docker compose exec postgres psql -U sales -d sales

# Ejecutar flow manualmente desde contenedor
docker compose exec prefect-worker python -m flows.master_flow
```

### VerificaciÃ³n de datos

```bash
# Contar transacciones
docker compose exec postgres psql -U sales -d sales -c "SELECT COUNT(*) FROM transactions;"

# Ver categorÃ­as
docker compose exec postgres psql -U sales -d sales -c "SELECT * FROM categories LIMIT 10;"

# Verificar archivos generados
ls -lh output/summary/

# Verificar sincronizaciÃ³n al frontend
ls -lh ../sales-frontend/public/data/
```

## ðŸ“˜ GuÃ­a de Prefect

### Â¿Por quÃ© Prefect?

Prefect proporciona:

- **UI web moderna** para monitoreo en tiempo real
- **RecuperaciÃ³n automÃ¡tica** de fallos con retries configurables
- **Logging centralizado** con trazabilidad completa
- **OrquestaciÃ³n flexible** de flujos complejos
- **EjecuciÃ³n manual o programada** segÃºn necesidad

### Flows Disponibles

1. **`data_loading_flow`**: Carga datos CSV a PostgreSQL automÃ¡ticamente
2. **`data_monitor_flow`**: ðŸ†• Monitorea y detecta nuevos datos en la base de datos
   - ðŸ¤– **Deployment automÃ¡tico**: Se ejecuta cada 2 minutos al configurar Prefect
3. **`executive_summary_flow`**: Genera mÃ©tricas ejecutivas
4. **`analytics_flow`**: AnÃ¡lisis temporal y correlaciones
5. **`clustering_flow`**: SegmentaciÃ³n de clientes con K-Means
6. **`recommendations_flow`**: Sistema de recomendaciones con FP-Growth
7. **`output_sync_flow`**: Sincroniza outputs JSON al frontend
8. **`master_flow`**: Ejecuta todos los pipelines en secuencia y sincroniza

### Monitoreo

Accede a http://localhost:4200 para:

- Ver ejecuciones en progreso y completadas
- Inspeccionar logs detallados por task
- Revisar duraciÃ³n y rendimiento
- Consultar errores y stack traces
- Visualizar el grafo de dependencias de tasks

### DocumentaciÃ³n Completa

Ver [PREFECT_GUIDE.md](PREFECT_GUIDE.md) para:

- Arquitectura detallada
- CÃ³mo crear nuevos flows
- ConfiguraciÃ³n de schedules
- Troubleshooting avanzado
- Mejores prÃ¡cticas

Ver [DATA_MONITOR_GUIDE.md](DATA_MONITOR_GUIDE.md) para:

- GuÃ­a completa del monitor de datos
- Ejemplos de uso avanzado
- ConfiguraciÃ³n de monitoreo continuo
- IntegraciÃ³n con webhooks y APIs
- Troubleshooting del monitor

## ðŸ“ Notas Importantes

- **Tiempo de procesamiento**: Los pipelines pueden tardar varios minutos dependiendo del tamaÃ±o de los datos y recursos disponibles
- **Persistencia**: Los datos en PostgreSQL persisten entre reinicios gracias a volÃºmenes Docker
- **Logs**: Se almacenan en `airflow/logs/` y persisten entre reinicios
- **Recursos**: Se recomienda al menos 8 GB de RAM para ejecutar todos los servicios simultÃ¡neamente
- **Resultados**: Los JSON generados estÃ¡n optimizados para consumo desde el frontend React

## ðŸ‘¥ Autores

- Juan David Colonia Aldana â€“ A00395956
- Miguel Ãngel Gonzalez Arango â€“ A00395687

## ðŸ§­ Contenido

- [ðŸ“Š Pipeline de AnÃ¡lisis de Ventas con Apache Spark](#-pipeline-de-anÃ¡lisis-de-ventas-con-apache-spark)
  - [ðŸ‘¥ Autores](#-autores)
  - [ðŸ§­ Contenido](#-contenido)
  - [ðŸ—‚ï¸ DescripciÃ³n de los datos](#ï¸-descripciÃ³n-de-los-datos)
  - [ðŸ”¬ MetodologÃ­a de anÃ¡lisis](#-metodologÃ­a-de-anÃ¡lisis)
  - [ðŸ“ˆ Principales hallazgos visuales](#-principales-hallazgos-visuales)
  - [ðŸ§  Resultados de modelos](#-resultados-de-modelos)
  - [ðŸŽ¯ Conclusiones y aplicaciones empresariales](#-conclusiones-y-aplicaciones-empresariales)
  - [ðŸ§µ Pipelines disponibles](#-pipelines-disponibles)
  - [âš™ï¸ EjecuciÃ³n de pipelines](#ï¸-ejecuciÃ³n-de-pipelines)
  - [Estructura del repositorio (carpeta `airflow/`)](#estructura-del-repositorio-carpeta-airflow)

---

## ðŸ—‚ï¸ DescripciÃ³n de los datos

Los datos originales provienen de transacciones minoristas:

- `transactions/`: archivos `*_Tran.csv` con el histÃ³rico de compras. Cada registro trae fecha, tienda, cliente y lista de productos.
- `Categories.csv` y `Product_Categories.csv`: catÃ¡logo de productos y su relaciÃ³n con categorÃ­as.
- Fuente: entregables del curso.

Se cargan en PostgreSQL mediante los scripts `scripts/linux/windows/load_data.*` y luego Spark accede vÃ­a JDBC para todos los pipelines.

---

## ðŸ”¬ MetodologÃ­a de anÃ¡lisis

1. **Ingesta**: Spark lee las tablas principales (`transactions`, `categories`, `product_categories`) directamente desde PostgreSQL.
2. **Enriquecimiento**: se explotan las listas de productos, se calculan mÃ©tricas por cliente, categorÃ­a y fecha, y se estandarizan fechas y tipos numÃ©ricos.
3. **AnÃ¡lisis ejecutivo**: agregaciones en Spark SQL para mÃ©tricas de negocio (transacciones, productos, top-N).
4. **AnalÃ­tica temporal**: series de tiempo diarias, semanales y mensuales; patrones por dÃ­a de semana; boxplots por categorÃ­a.
5. **Modelos avanzados**:
   - **Clustering**: K-Means con variables de frecuencia, volumen y diversidad de compra.
   - **Recomendaciones**: FP-Growth para reglas de asociaciÃ³n. Se generan salidas por producto y por cliente.
6. **ExportaciÃ³n**: todos los resultados se escriben como JSON en `output/` para ser consumidos por el frontend.

---

## ðŸ“ˆ Principales hallazgos visuales

- **Patrones temporales**: se observan picos los fines de semana y un comportamiento mensual con ligeras estacionalidades.
- **Boxplots por categorÃ­a**: algunas familias (por ejemplo, â€œFrutas y verdurasâ€) concentran la mayor parte de unidades, mientras categorÃ­as especializadas tienen variaciones menores.
- **Heatmap**: la correlaciÃ³n revela que frecuencia de compra y diversidad de productos estÃ¡n positivamente relacionadas con el volumen total vendido.

Los archivos JSON en `output/analytics/` contienen las series y distribuciones que alimentan las grÃ¡ficas del dashboard React.

---

## ðŸ§  Resultados de modelos

- **SegmentaciÃ³n (K-Means)**  
  Se generan cuatro clusters con perfiles claros:

  1. **VIP/Premium**: alta frecuencia y volumen; candidatos a programas de fidelizaciÃ³n robustos.
  2. **Exploradores**: compran gran variedad de productos/categorÃ­as; responden bien a lanzamientos.
  3. **Ocasionales**: pocas compras al aÃ±o; requieren campaÃ±as de reactivaciÃ³n.
  4. **Clientes nuevos**: transacciones recientes y de bajo volumen; conviene guiarlos hacia categorÃ­as rentables.

- **Recomendaciones (FP-Growth)**
  - **Producto â†’ producto**: reglas con lift > 3 identifican complementos naturales (ej. categorÃ­as frescas + abarrotes).
  - **Cliente â†’ producto**: sugerencias personalizadas derivadas de las reglas y del historial individual.
  - Se guardan estadÃ­sticas del dataset para evitar recalcular FP-Growth si no cambian los datos.

---

## ðŸŽ¯ Conclusiones y aplicaciones empresariales

- El pipeline permite monitorear el negocio a nivel ejecutivo, detectar patrones temporales y segmentar clientes sin depender de herramientas externas.
- Las reglas de asociaciÃ³n alimentan estrategias de cross-selling tanto en tienda como en canales digitales.
- Los clusters facilitan campaÃ±as especÃ­ficas: retenciÃ³n de VIPs, incentivos a exploradores, reactivaciÃ³n de clientes ocasionales.
- Exportar en JSON permite integrar fÃ¡cilmente un dashboard React o cualquier otra aplicaciÃ³n que consuma APIs o archivos estÃ¡ticos.

---

## ðŸ§µ Pipelines disponibles

| Pipeline            | Objetivo                                                                                                                                  | Salidas principales                                  |
| ------------------- | ----------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------- |
| `executive_summary` | KPIs ejecutivos: totales de transacciones y ventas, top 10 de productos, clientes y categorÃ­as, dÃ­as pico.                                | `output/summary/basic_metrics.json`, `top_10_*.json` |
| `analytics`         | Series de tiempo (diaria/semanal/mensual), patrones por dÃ­a de la semana, boxplots por categorÃ­a y heatmap de correlaciones.              | `output/analytics/*.json`                            |
| `clustering`        | SegmentaciÃ³n K-Means en cuatro perfiles (VIP, exploradores, ocasionales, nuevos) con mÃ©tricas y recomendaciones por cluster.              | `output/advanced/clustering/*.json`                  |
| `recommendations`   | Reglas de asociaciÃ³n FP-Growth + sugerencias por producto y por cliente. Reutiliza resultados si las estadÃ­sticas del dataset no cambian. | `output/advanced/recommendations/*.json`             |

---

## âš™ï¸ EjecuciÃ³n de pipelines

1. **Levantar servicios** (Spark master/worker, PostgreSQL, cliente):

   ```bash
   docker compose up -d
   ```

2. **Cargar datos** (si es la primera vez):

   ```bash
   ./scripts/linux/load_data.sh ../data
   # o en Windows
   scripts/windows/load_data.bat ..\data
   ```

3. **Ejecutar pipelines** dentro del contenedor `spark-client`:

   ```bash
   docker compose exec spark-client python src/run_pipeline.py executive_summary
   docker compose exec spark-client python src/run_pipeline.py analytics
   docker compose exec spark-client python src/run_pipeline.py clustering --n-clusters 4
   docker compose exec spark-client python src/run_pipeline.py recommendations
   ```

4. **Reutilizar FP-Growth**: si los stats de canastas no cambian, el pipeline de recomendaciones reutiliza los resultados previos (`output/data/fp_growth_*`).

5. **Copiar salidas al frontend**: mover `output/summary`, `output/analytics`, `output/advanced/*`, `output/advanced/recommendations` a `sales-frontend/public/data/`.

---

## Estructura del repositorio (carpeta `airflow/`)

```
airflow/
â”œâ”€â”€ docker-compose.yml          # Servicios Spark + PostgreSQL + cliente
â”œâ”€â”€ requirements.txt            # Dependencias (Spark, pandas, sklearn, etc.)
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ run_pipeline.py         # CLI para ejecutar pipelines Spark
â”‚   â”œâ”€â”€ config/spark_config.py  # ConfiguraciÃ³n del SparkSession
â”‚   â”œâ”€â”€ pipelines/              # Pipelines: executive, analytics, clustering, recommendations
â”‚   â”œâ”€â”€ analyzers/              # LÃ³gica de mÃ©tricas, estadÃ­stica y modelos
â”‚   â”œâ”€â”€ data_loader.py          # Lectura JDBC desde PostgreSQL
â”‚   â””â”€â”€ json_exporter.py        # Utilidades para escribir JSON
â”œâ”€â”€ scripts/                    # Scripts para cargar datos y ejecutar pipelines (Linux/Windows)
â”œâ”€â”€ data/                       # CSV originales (montar localmente)
â””â”€â”€ output/                     # Resultados JSON para el frontend
```

> **Nota**: Esta carpeta se llama `airflow` por el contexto original, pero hoy los pipelines se ejecutan directamente sobre Spark con scripts Dockerizados. No se requiere un scheduler externo.

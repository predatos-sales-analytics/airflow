# ğŸš€ Pipeline de AnÃ¡lisis de Ventas con Prefect

Sistema de anÃ¡lisis de ventas distribuido que utiliza **Prefect** para orquestar pipelines de procesamiento con Apache Spark, generando mÃ©tricas ejecutivas, anÃ¡lisis temporal, segmentaciÃ³n de clientes y recomendaciones de productos.

> **Nota**: Este proyecto ha sido migrado de Apache Airflow a Prefect para mejorar la flexibilidad, observabilidad y facilidad de uso. Ver [PREFECT_GUIDE.md](PREFECT_GUIDE.md) para mÃ¡s detalles.

## ğŸ“‹ Tabla de Contenidos

- [Requisitos](#requisitos)
- [Inicio RÃ¡pido con Prefect](#inicio-rÃ¡pido-con-prefect)
- [Pipelines Disponibles](#pipelines-disponibles)
- [Estructura del Proyecto](#estructura-del-proyecto)
- [ConfiguraciÃ³n Avanzada](#configuraciÃ³n-avanzada)
- [GuÃ­a de Prefect](#guÃ­a-de-prefect)
- [Troubleshooting](#troubleshooting)

## ğŸ”§ Requisitos

- **Docker** y **Docker Compose** instalados
- **Git** (para clonar el repositorio)
- Al menos **8 GB de RAM** disponibles para los contenedores
- **Puerto 4200** libre para Prefect UI
- **Puerto 5432** libre para PostgreSQL

## âš¡ Inicio RÃ¡pido con Prefect

### Paso 1: Levantar los servicios

Desde el directorio `airflow/`:

```bash
docker compose up -d
```

Esto iniciarÃ¡:

- **PostgreSQL** (base de datos)
- **Prefect Server** (orquestador - UI en puerto 4200)
- **Prefect Worker** (ejecutor de flows)
- **Spark Master y Worker** (procesamiento distribuido)

**Tiempo estimado**: 2-3 minutos

Verifica que todos los servicios estÃ©n corriendo:

```bash
docker compose ps
```

### Paso 1.5: Configurar Prefect (primera vez)

DespuÃ©s de levantar los servicios por primera vez, configura Prefect:

#### En Linux/Mac:

```bash
./scripts/linux/setup_prefect.sh
```

#### En Windows:

```cmd
scripts\windows\setup_prefect.bat
```

Esto crearÃ¡ el work pool necesario. Accede a la UI de Prefect en: **http://localhost:4200**

### Paso 2: Cargar los datos

Ahora puedes cargar los datos usando el **flow de Prefect** (automatizado) o los scripts tradicionales.

#### OpciÃ³n A: Usando Prefect Flow (Recomendado)

##### En Linux/Mac:

```bash
./scripts/linux/run_prefect_flow.sh data_loading
```

##### En Windows:

```cmd
scripts\windows\run_prefect_flow.bat data_loading
```

#### OpciÃ³n B: Scripts tradicionales

##### En Linux/Mac:

```bash
./scripts/linux/load_data.sh
```

##### En Windows:

```cmd
scripts\windows\load_data.bat
```

**Nota**: Los archivos CSV deben estar en el directorio `data/` con la estructura:

- `data/products/Categories.csv`
- `data/products/ProductCategory.csv`
- `data/transactions/*.csv`

**Tiempo estimado**: 5-10 minutos (depende del tamaÃ±o de los datos)

### Paso 3: Ejecutar los pipelines

Una vez cargados los datos, ejecuta los pipelines de anÃ¡lisis usando Prefect.

#### Ejecutar todos los pipelines (Flow Maestro)

Este flow ejecuta todos los pipelines en secuencia y sincroniza los resultados al frontend automÃ¡ticamente.

##### En Linux/Mac:

```bash
./scripts/linux/run_prefect_flow.sh master
```

##### En Windows:

```cmd
scripts\windows\run_prefect_flow.bat master
```

#### Ejecutar pipelines individuales

##### En Linux/Mac:

```bash
./scripts/linux/run_prefect_flow.sh executive_summary
./scripts/linux/run_prefect_flow.sh analytics
./scripts/linux/run_prefect_flow.sh clustering
./scripts/linux/run_prefect_flow.sh recommendations
./scripts/linux/run_prefect_flow.sh output_sync  # Sincronizar al frontend
```

##### En Windows:

```cmd
scripts\windows\run_prefect_flow.bat executive_summary
scripts\windows\run_prefect_flow.bat analytics
scripts\windows\run_prefect_flow.bat clustering
scripts\windows\run_prefect_flow.bat recommendations
scripts\windows\run_prefect_flow.bat output_sync
```

**Tiempo estimado**:

- Resumen ejecutivo: 3-5 minutos
- AnalÃ­tica temporal: 5-8 minutos
- Clustering: 8-12 minutos
- Recomendaciones: 10-15 minutos
- Flow maestro completo: 30-45 minutos

**Monitoreo**: Accede a http://localhost:4200 para ver el progreso en tiempo real

### Paso 4: Visualizar resultados

Los resultados se generan en formato JSON en el directorio `output/` y se sincronizan automÃ¡ticamente al frontend si ejecutaste el `master_flow` o el `output_sync_flow`.

```
output/
â”œâ”€â”€ summary/              # MÃ©tricas ejecutivas
â”‚   â”œâ”€â”€ basic_metrics.json
â”‚   â”œâ”€â”€ top_10_products.json
â”‚   â””â”€â”€ top_10_customers.json
â”œâ”€â”€ analytics/            # Series temporales y correlaciones
â”‚   â”œâ”€â”€ daily_sales.json
â”‚   â””â”€â”€ variable_correlation.json
â”œâ”€â”€ advanced/
â”‚   â””â”€â”€ clustering/       # SegmentaciÃ³n de clientes
â”‚       â”œâ”€â”€ cluster_summary.json
â”‚       â””â”€â”€ clustering_visualization.json
â””â”€â”€ recommendations/      # Recomendaciones de productos
    â”œâ”€â”€ product_recs.json
    â””â”€â”€ customer_recs.json
```

**SincronizaciÃ³n automÃ¡tica**: Si ejecutaste el flow maestro o el `output_sync_flow`, los archivos ya estÃ¡n en `../sales-frontend/public/data/`

**Para visualizar**: Ejecuta el dashboard React desde el directorio `sales-frontend/`

## ğŸ“Š Pipelines Disponibles

### 1. Resumen Ejecutivo (`executive_summary`)

Genera mÃ©tricas clave del negocio:

- Total de transacciones y productos vendidos
- Top 10 productos mÃ¡s vendidos
- Top 10 clientes mÃ¡s activos
- Top 10 categorÃ­as por volumen
- DÃ­as pico de ventas

**Salida**: `output/summary/*.json`

### 2. AnalÃ­tica Temporal (`analytics`)

AnÃ¡lisis de patrones temporales y correlaciones:

- Series de tiempo (diarias, semanales, mensuales)
- Patrones por dÃ­a de la semana
- DistribuciÃ³n de productos por categorÃ­a y tienda (boxplot)
- Matriz de correlaciÃ³n entre variables

**Salida**: `output/analytics/*.json`

### 3. SegmentaciÃ³n de Clientes (`clustering`)

Clustering K-Means para identificar perfiles de clientes:

- 4 clusters: VIP/Premium, Exploradores, Ocasionales, Nuevos
- MÃ©tricas por cluster (frecuencia, volumen, diversidad)
- Recomendaciones de negocio por segmento
- VisualizaciÃ³n de clasificaciÃ³n (scatter plot)

**ParÃ¡metros**:

- `--n-clusters`: NÃºmero de clusters (default: 4)

**Salida**: `output/advanced/clustering/*.json`

### 4. Recomendaciones (`recommendations`)

Sistema de recomendaciones basado en reglas de asociaciÃ³n (FP-Growth):

- **Por producto**: Productos complementarios que suelen comprarse juntos
- **Por cliente**: Sugerencias personalizadas segÃºn historial de compra

**ParÃ¡metros**:

- `--min-support`: Soporte mÃ­nimo para reglas (default: 0.005)
- `--min-confidence`: Confianza mÃ­nima para reglas (default: 0.2)

**Salida**: `output/recommendations/*.json`

## ğŸ“ Estructura del Proyecto

```
airflow/
â”œâ”€â”€ docker-compose.yml           # OrquestaciÃ³n de servicios (incluye Prefect)
â”œâ”€â”€ requirements.txt             # Dependencias Python (incluye Prefect)
â”œâ”€â”€ env.template                 # Template de variables de entorno
â”œâ”€â”€ PREFECT_GUIDE.md             # GuÃ­a detallada de Prefect
â”‚
â”œâ”€â”€ src/                         # CÃ³digo fuente
â”‚   â”œâ”€â”€ run_pipeline.py          # CLI para ejecutar pipelines (legacy)
â”‚   â”œâ”€â”€ prefect_config.py        # ConfiguraciÃ³n de Prefect
â”‚   â”‚
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â””â”€â”€ spark_config.py      # ConfiguraciÃ³n de Spark
â”‚   â”‚
â”‚   â”œâ”€â”€ flows/                   # ğŸ†• Flows de Prefect
â”‚   â”‚   â”œâ”€â”€ data_loading_flow.py           # Flow de carga de datos
â”‚   â”‚   â”œâ”€â”€ executive_summary_flow.py      # Flow resumen ejecutivo
â”‚   â”‚   â”œâ”€â”€ analytics_flow.py              # Flow anÃ¡lisis temporal
â”‚   â”‚   â”œâ”€â”€ clustering_flow.py             # Flow clustering
â”‚   â”‚   â”œâ”€â”€ recommendations_flow.py        # Flow recomendaciones
â”‚   â”‚   â”œâ”€â”€ output_sync_flow.py            # Flow sincronizaciÃ³n frontend
â”‚   â”‚   â”œâ”€â”€ master_flow.py                 # Flow maestro (orquesta todo)
â”‚   â”‚   â””â”€â”€ notifications.py               # Sistema de notificaciones
â”‚   â”‚
â”‚   â”œâ”€â”€ pipelines/               # Pipelines principales (lÃ³gica de negocio)
â”‚   â”‚   â”œâ”€â”€ executive_summary_pipeline.py
â”‚   â”‚   â”œâ”€â”€ analytics_pipeline.py
â”‚   â”‚   â”œâ”€â”€ clustering_pipeline.py
â”‚   â”‚   â””â”€â”€ recommendations_pipeline.py
â”‚   â”‚
â”‚   â”œâ”€â”€ analyzers/               # MÃ³dulos de anÃ¡lisis
â”‚   â”‚   â”œâ”€â”€ summary_metrics.py
â”‚   â”‚   â”œâ”€â”€ temporal_analyzer.py
â”‚   â”‚   â”œâ”€â”€ customer_analyzer.py
â”‚   â”‚   â””â”€â”€ product_analyzer.py
â”‚   â”‚
â”‚   â”œâ”€â”€ data_loader.py           # Carga de datos desde PostgreSQL
â”‚   â””â”€â”€ json_exporter.py         # ExportaciÃ³n de resultados
â”‚
â”œâ”€â”€ scripts/                     # Scripts de utilidad
â”‚   â”œâ”€â”€ linux/
â”‚   â”‚   â”œâ”€â”€ load_data.sh         # Carga de datos (Linux/Mac)
â”‚   â”‚   â”œâ”€â”€ setup_prefect.sh     # ğŸ†• Configurar Prefect
â”‚   â”‚   â””â”€â”€ run_prefect_flow.sh  # ğŸ†• Ejecutar flows de Prefect
â”‚   â””â”€â”€ windows/
â”‚       â”œâ”€â”€ load_data.bat        # Carga de datos (Windows)
â”‚       â”œâ”€â”€ setup_prefect.bat    # ğŸ†• Configurar Prefect
â”‚       â””â”€â”€ run_prefect_flow.bat # ğŸ†• Ejecutar flows de Prefect
â”‚
â”œâ”€â”€ data/                        # Datos CSV (montar aquÃ­)
â”‚   â”œâ”€â”€ Categories.csv
â”‚   â”œâ”€â”€ Product_Categories.csv
â”‚   â””â”€â”€ transactions/
â”‚
â”œâ”€â”€ output/                      # Resultados generados
â”‚   â”œâ”€â”€ summary/
â”‚   â”œâ”€â”€ analytics/
â”‚   â”œâ”€â”€ advanced/
â”‚   â””â”€â”€ recommendations/
â”‚
â”œâ”€â”€ docker/                      # ConfiguraciÃ³n Docker
â”‚   â”œâ”€â”€ prefect-worker/Dockerfile  # ğŸ†• Dockerfile para Prefect Worker
â”‚   â”œâ”€â”€ spark-client/Dockerfile
â”‚   â”œâ”€â”€ spark-worker/Dockerfile
â”‚   â””â”€â”€ postgres/init-sales-db.sh
â”‚
â””â”€â”€ logs/                        # Logs y ejecuciones
    â””â”€â”€ prefect_runs/            # ğŸ†• Logs de flows de Prefect
```

## âš™ï¸ ConfiguraciÃ³n Avanzada

### Variables de Entorno

Crea un archivo `.env` basado en `env.template`:

```bash
cp env.template .env
```

**Variables principales**:

```bash
# PostgreSQL
POSTGRES_USER=sales
POSTGRES_PASSWORD=sales
POSTGRES_DB=sales

# Prefect
PREFECT_API_URL=http://prefect-server:4200/api

# Spark
SPARK_MASTER_URL=spark://spark-master:7077

# Paths
DATA_PATH=/opt/prefect/work-dir/data
OUTPUT_PATH=/opt/prefect/work-dir/output
FRONTEND_PATH=/opt/prefect/work-dir/frontend
```

### Acceso a Interfaces Web

Una vez levantados los servicios:

- **Prefect UI**: http://localhost:4200
  - Dashboard de flows, ejecuciones, logs y monitoreo
- **Spark Master UI**: http://localhost:8082
  - Monitoreo de jobs de Spark

### ConfiguraciÃ³n de Spark

Edita `src/config/spark_config.py` para ajustar:

- Memoria del driver y executors
- NÃºmero de cores
- Particiones de shuffle

### ParÃ¡metros de Pipelines con Prefect

Los flows de Prefect aceptan parÃ¡metros. Aunque actualmente se ejecutan con valores por defecto desde los scripts, puedes modificarlos en el cÃ³digo de los flows o crear deployments personalizados.

**Valores por defecto:**

- Clustering: `n_clusters=4`
- Recomendaciones: `min_support=0.005`, `min_confidence=0.2`
- Master Flow: ejecuta todos con los valores por defecto y sincroniza al frontend

Ver [PREFECT_GUIDE.md](PREFECT_GUIDE.md) para personalizaciÃ³n avanzada.

## ğŸ› ï¸ Comandos Ãštiles

### GestiÃ³n de servicios

```bash
# Iniciar servicios
docker compose up -d

# Detener servicios
docker compose down

# Ver logs en tiempo real (todos los servicios)
docker compose logs -f

# Ver logs de Prefect
docker compose logs -f prefect-server prefect-worker

# Reiniciar un servicio especÃ­fico
docker compose restart prefect-worker

# Limpiar todo (incluyendo volÃºmenes)
docker compose down -v
```

### Acceso a contenedores

```bash
# Acceder a shell de Prefect Worker
docker compose exec prefect-worker bash

# Acceder a PostgreSQL
docker compose exec postgres psql -U sales -d sales

# Ejecutar flow manualmente desde contenedor
docker compose exec prefect-worker python -m flows.master_flow
```

### VerificaciÃ³n de datos

```bash
# Contar transacciones
docker compose exec postgres psql -U sales -d sales -c "SELECT COUNT(*) FROM transactions;"

# Ver categorÃ­as
docker compose exec postgres psql -U sales -d sales -c "SELECT * FROM categories LIMIT 10;"

# Verificar archivos generados
ls -lh output/summary/

# Verificar sincronizaciÃ³n al frontend
ls -lh ../sales-frontend/public/data/
```

## ğŸ“˜ GuÃ­a de Prefect

### Â¿Por quÃ© Prefect?

Prefect proporciona:

- **UI web moderna** para monitoreo en tiempo real
- **RecuperaciÃ³n automÃ¡tica** de fallos con retries configurables
- **Logging centralizado** con trazabilidad completa
- **OrquestaciÃ³n flexible** de flujos complejos
- **EjecuciÃ³n manual o programada** segÃºn necesidad

### Flows Disponibles

1. **`data_loading_flow`**: Carga datos CSV a PostgreSQL automÃ¡ticamente
2. **`executive_summary_flow`**: Genera mÃ©tricas ejecutivas
3. **`analytics_flow`**: AnÃ¡lisis temporal y correlaciones
4. **`clustering_flow`**: SegmentaciÃ³n de clientes con K-Means
5. **`recommendations_flow`**: Sistema de recomendaciones con FP-Growth
6. **`output_sync_flow`**: Sincroniza outputs JSON al frontend
7. **`master_flow`**: Ejecuta todos los pipelines en secuencia y sincroniza

### Monitoreo

Accede a http://localhost:4200 para:

- Ver ejecuciones en progreso y completadas
- Inspeccionar logs detallados por task
- Revisar duraciÃ³n y rendimiento
- Consultar errores y stack traces
- Visualizar el grafo de dependencias de tasks

### DocumentaciÃ³n Completa

Ver [PREFECT_GUIDE.md](PREFECT_GUIDE.md) para:

- Arquitectura detallada
- CÃ³mo crear nuevos flows
- ConfiguraciÃ³n de schedules
- Troubleshooting avanzado
- Mejores prÃ¡cticas

## ğŸ“ Notas Importantes

- **Tiempo de procesamiento**: Los pipelines pueden tardar varios minutos dependiendo del tamaÃ±o de los datos y recursos disponibles
- **Persistencia**: Los datos en PostgreSQL persisten entre reinicios gracias a volÃºmenes Docker
- **Logs**: Se almacenan en `airflow/logs/` y persisten entre reinicios
- **Recursos**: Se recomienda al menos 8 GB de RAM para ejecutar todos los servicios simultÃ¡neamente
- **Resultados**: Los JSON generados estÃ¡n optimizados para consumo desde el frontend React

## ğŸ‘¥ Autores

- Juan David Colonia Aldana - A00395956
- Miguel Ãngel Gonzalez Arango - A00395687

# ğŸš€ Pipeline de AnÃ¡lisis de Ventas con Apache Airflow

Sistema de anÃ¡lisis de ventas distribuido que utiliza Apache Airflow para orquestar pipelines de procesamiento con Apache Spark, generando mÃ©tricas ejecutivas, anÃ¡lisis temporal, segmentaciÃ³n de clientes y recomendaciones de productos.

## ğŸ“‹ Tabla de Contenidos

- [Requisitos](#requisitos)
- [Inicio RÃ¡pido](#inicio-rÃ¡pido)
- [Pipelines Disponibles](#pipelines-disponibles)
- [Estructura del Proyecto](#estructura-del-proyecto)
- [ConfiguraciÃ³n Avanzada](#configuraciÃ³n-avanzada)
- [Troubleshooting](#troubleshooting)

## ğŸ”§ Requisitos

- **Docker** y **Docker Compose** instalados
- **Git** (para clonar el repositorio)
- Al menos **8 GB de RAM** disponibles para los contenedores
- **Puerto 8085** libre para Airflow UI

## âš¡ Inicio RÃ¡pido

### Paso 1: Levantar los servicios

Desde el directorio `airflow/`:

```bash
docker compose up -d
```

Esto iniciarÃ¡:

- PostgreSQL (base de datos)
- Apache Airflow (scheduler, webserver, worker)
- Spark Master y Worker (procesamiento distribuido)

**Tiempo estimado**: 2-3 minutos

Verifica que todos los servicios estÃ©n corriendo:

```bash
docker compose ps
```

### Paso 2: Cargar los datos

Una vez que los servicios estÃ©n activos, carga los datos en PostgreSQL.

#### En Linux/Mac:

```bash
./scripts/load_data.sh ../data
```

#### En Windows:

```cmd
scripts\windows\load_data.bat ..\data
```

**Nota**: Ajusta la ruta `../data` segÃºn la ubicaciÃ³n de tus archivos CSV (`Categories.csv`, `Product_Categories.csv`, `transactions/`).

**Tiempo estimado**: 5-10 minutos (depende del tamaÃ±o de los datos)

### Paso 3: Ejecutar los pipelines

Una vez cargados los datos, ejecuta los pipelines de anÃ¡lisis.

#### En Linux/Mac:

```bash
# Ejecutar todos los pipelines
./scripts/run_all_pipelines.sh

# O ejecutar pipelines individuales
cd src
python run_pipeline.py executive_summary
python run_pipeline.py analytics
python run_pipeline.py clustering --n-clusters 4
python run_pipeline.py recommendations --min-support 0.005 --min-confidence 0.2
```

#### En Windows:

```cmd
REM Ejecutar todos los pipelines
scripts\windows\run_pipeline_docker.bat all

REM O ejecutar pipelines individuales
scripts\windows\run_pipeline_docker.bat executive_summary
scripts\windows\run_pipeline_docker.bat analytics
scripts\windows\run_pipeline_docker.bat clustering
scripts\windows\run_pipeline_docker.bat recommendations
```

**Tiempo estimado**:

- Resumen ejecutivo: 3-5 minutos
- AnalÃ­tica temporal: 5-8 minutos
- Clustering: 8-12 minutos
- Recomendaciones: 10-15 minutos

### Paso 4: Visualizar resultados

Los resultados se generan en formato JSON en el directorio `output/`:

```
output/
â”œâ”€â”€ summary/              # MÃ©tricas ejecutivas
â”‚   â”œâ”€â”€ basic_metrics.json
â”‚   â”œâ”€â”€ top_10_products.json
â”‚   â””â”€â”€ top_10_customers.json
â”œâ”€â”€ analytics/            # Series temporales y correlaciones
â”‚   â”œâ”€â”€ daily_sales.json
â”‚   â””â”€â”€ variable_correlation.json
â”œâ”€â”€ advanced/
â”‚   â””â”€â”€ clustering/       # SegmentaciÃ³n de clientes
â”‚       â”œâ”€â”€ cluster_summary.json
â”‚       â””â”€â”€ clustering_visualization.json
â””â”€â”€ recommendations/      # Recomendaciones de productos
    â”œâ”€â”€ product_recs.json
    â””â”€â”€ customer_recs.json
```

**Para visualizar en el frontend**: Copia los archivos JSON a `sales-frontend/public/data/` y ejecuta el dashboard React.

## ğŸ“Š Pipelines Disponibles

### 1. Resumen Ejecutivo (`executive_summary`)

Genera mÃ©tricas clave del negocio:

- Total de transacciones y productos vendidos
- Top 10 productos mÃ¡s vendidos
- Top 10 clientes mÃ¡s activos
- Top 10 categorÃ­as por volumen
- DÃ­as pico de ventas

**Salida**: `output/summary/*.json`

### 2. AnalÃ­tica Temporal (`analytics`)

AnÃ¡lisis de patrones temporales y correlaciones:

- Series de tiempo (diarias, semanales, mensuales)
- Patrones por dÃ­a de la semana
- DistribuciÃ³n de productos por categorÃ­a y tienda (boxplot)
- Matriz de correlaciÃ³n entre variables

**Salida**: `output/analytics/*.json`

### 3. SegmentaciÃ³n de Clientes (`clustering`)

Clustering K-Means para identificar perfiles de clientes:

- 4 clusters: VIP/Premium, Exploradores, Ocasionales, Nuevos
- MÃ©tricas por cluster (frecuencia, volumen, diversidad)
- Recomendaciones de negocio por segmento
- VisualizaciÃ³n de clasificaciÃ³n (scatter plot)

**ParÃ¡metros**:

- `--n-clusters`: NÃºmero de clusters (default: 4)

**Salida**: `output/advanced/clustering/*.json`

### 4. Recomendaciones (`recommendations`)

Sistema de recomendaciones basado en reglas de asociaciÃ³n (FP-Growth):

- **Por producto**: Productos complementarios que suelen comprarse juntos
- **Por cliente**: Sugerencias personalizadas segÃºn historial de compra

**ParÃ¡metros**:

- `--min-support`: Soporte mÃ­nimo para reglas (default: 0.005)
- `--min-confidence`: Confianza mÃ­nima para reglas (default: 0.2)

**Salida**: `output/recommendations/*.json`

## ğŸ“ Estructura del Proyecto

```
airflow/
â”œâ”€â”€ docker-compose.yml           # OrquestaciÃ³n de servicios
â”œâ”€â”€ requirements.txt             # Dependencias Python
â”œâ”€â”€ env.template                 # Template de variables de entorno
â”‚
â”œâ”€â”€ src/                         # CÃ³digo fuente
â”‚   â”œâ”€â”€ run_pipeline.py          # CLI para ejecutar pipelines
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â””â”€â”€ spark_config.py      # ConfiguraciÃ³n de Spark
â”‚   â”œâ”€â”€ pipelines/               # Pipelines principales
â”‚   â”‚   â”œâ”€â”€ executive_summary_pipeline.py
â”‚   â”‚   â”œâ”€â”€ analytics_pipeline.py
â”‚   â”‚   â”œâ”€â”€ clustering_pipeline.py
â”‚   â”‚   â””â”€â”€ recommendations_pipeline.py
â”‚   â”œâ”€â”€ analyzers/               # MÃ³dulos de anÃ¡lisis
â”‚   â”‚   â”œâ”€â”€ summary_metrics.py
â”‚   â”‚   â”œâ”€â”€ temporal_analyzer.py
â”‚   â”‚   â”œâ”€â”€ customer_analyzer.py
â”‚   â”‚   â””â”€â”€ product_analyzer.py
â”‚   â”œâ”€â”€ data_loader.py           # Carga de datos desde PostgreSQL
â”‚   â””â”€â”€ json_exporter.py         # ExportaciÃ³n de resultados
â”‚
â”œâ”€â”€ scripts/                     # Scripts de utilidad
â”‚   â”œâ”€â”€ load_data.sh             # Carga de datos (Linux/Mac)
â”‚   â””â”€â”€ windows/
â”‚       â”œâ”€â”€ load_data.bat        # Carga de datos (Windows)
â”‚       â””â”€â”€ run_pipeline_docker.bat  # Ejecutar pipelines (Windows)
â”‚
â”œâ”€â”€ data/                        # Datos CSV (montar aquÃ­)
â”‚   â”œâ”€â”€ Categories.csv
â”‚   â”œâ”€â”€ Product_Categories.csv
â”‚   â””â”€â”€ transactions/
â”‚
â”œâ”€â”€ output/                      # Resultados generados
â”‚   â”œâ”€â”€ summary/
â”‚   â”œâ”€â”€ analytics/
â”‚   â”œâ”€â”€ advanced/
â”‚   â””â”€â”€ recommendations/
â”‚
â”œâ”€â”€ docker/                      # ConfiguraciÃ³n Docker
â”‚   â”œâ”€â”€ airflow/Dockerfile
â”‚   â””â”€â”€ postgres/init-sales-db.sh
â”‚
â””â”€â”€ logs/                        # Logs de Airflow
```

## âš™ï¸ ConfiguraciÃ³n Avanzada

### Variables de Entorno

Crea un archivo `.env` basado en `env.template`:

```bash
cp env.template .env
```

**Variables principales**:

```bash
# PostgreSQL
POSTGRES_USER=sales
POSTGRES_PASSWORD=sales
POSTGRES_DB=sales

# Airflow
AIRFLOW_UID=50000
AIRFLOW_FERNET_KEY=<generar con: python -c "from cryptography.fernet import Fernet; print(Fernet.generate_key().decode())">

# Spark
SPARK_MASTER_URL=spark://spark-master:7077
```

### Acceso a Interfaces Web

Una vez levantados los servicios:

- **Airflow UI**: http://localhost:8085
  - Usuario: `admin`
  - ContraseÃ±a: `admin`
- **Spark Master UI**: http://localhost:8080

### ConfiguraciÃ³n de Spark

Edita `src/config/spark_config.py` para ajustar:

- Memoria del driver y executors
- NÃºmero de cores
- Particiones de shuffle

### ParÃ¡metros de Pipelines

#### Clustering

```bash
python run_pipeline.py clustering --n-clusters 5
```

#### Recomendaciones

```bash
python run_pipeline.py recommendations \
  --min-support 0.01 \
  --min-confidence 0.3
```

## ğŸ› ï¸ Comandos Ãštiles

### GestiÃ³n de servicios

```bash
# Iniciar servicios
docker compose up -d

# Detener servicios
docker compose down

# Ver logs en tiempo real
docker compose logs -f

# Reiniciar un servicio especÃ­fico
docker compose restart airflow-scheduler

# Limpiar todo (incluyendo volÃºmenes)
docker compose down -v
```

### Acceso a contenedores

```bash
# Acceder a shell de Airflow
docker compose exec airflow-scheduler bash

# Acceder a PostgreSQL
docker compose exec postgres psql -U sales -d sales

# Ejecutar comando Python en Airflow
docker compose exec airflow-scheduler python /opt/airflow/src/run_pipeline.py --help
```

### VerificaciÃ³n de datos

```bash
# Contar transacciones
docker compose exec postgres psql -U sales -d sales -c "SELECT COUNT(*) FROM transactions;"

# Ver categorÃ­as
docker compose exec postgres psql -U sales -d sales -c "SELECT * FROM categories LIMIT 10;"

# Verificar archivos generados
ls -lh output/summary/
```

## ğŸ“ Notas Importantes

- **Tiempo de procesamiento**: Los pipelines pueden tardar varios minutos dependiendo del tamaÃ±o de los datos y recursos disponibles
- **Persistencia**: Los datos en PostgreSQL persisten entre reinicios gracias a volÃºmenes Docker
- **Logs**: Se almacenan en `airflow/logs/` y persisten entre reinicios
- **Recursos**: Se recomienda al menos 8 GB de RAM para ejecutar todos los servicios simultÃ¡neamente
- **Resultados**: Los JSON generados estÃ¡n optimizados para consumo desde el frontend React

## ğŸ‘¥ Autores

- Juan David Colonia Aldana - A00395956
- Miguel Ãngel Gonzalez Arango - A00395687

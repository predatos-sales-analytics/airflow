# ğŸš€ Pipeline de AnÃ¡lisis de Ventas con Apache Airflow

Pipeline de anÃ¡lisis de ventas orquestado con Apache Airflow, ejecutando anÃ¡lisis distribuidos con Apache Spark y procesamiento paralelo por tienda.

## ğŸ“‹ Tabla de Contenidos

- [Requisitos](#requisitos)
- [InstalaciÃ³n](#instalaciÃ³n)
- [ConfiguraciÃ³n](#configuraciÃ³n)
- [Uso](#uso)
- [Estructura del Proyecto](#estructura-del-proyecto)
- [DAGs Disponibles](#dags-disponibles)
- [Carga de Datos](#carga-de-datos)
- [Troubleshooting](#troubleshooting)

## ğŸ”§ Requisitos

- Docker y Docker Compose
- Git (para clonar este repositorio)

## ğŸ“¦ InstalaciÃ³n

### 1. Configurar variables de entorno

```bash
# Copiar template de variables de entorno
cp env.template .env

# Editar .env con tus valores
# Generar AIRFLOW_FERNET_KEY si no existe:
python -c "from cryptography.fernet import Fernet; print(Fernet.generate_key().decode())"
```

### 3. Inicializar Airflow

```bash
# Crear usuario y base de datos de Airflow
docker compose up airflow-init

# Esperar a que termine (verÃ¡s "User 'admin' created" al finalizar)
```

### 4. Levantar servicios

```bash
# Iniciar todos los servicios
docker compose up -d

# Ver logs
docker compose logs -f
```

## âš™ï¸ ConfiguraciÃ³n

### Variables de Entorno (.env)

El archivo `.env` contiene todas las configuraciones necesarias. Ver `.env.template` para referencia.

**Variables crÃ­ticas:**

- `AIRFLOW_UID`: ID de usuario para Airflow (generado automÃ¡ticamente en Linux/Mac)
- `AIRFLOW_FERNET_KEY`: Clave de encriptaciÃ³n para Airflow (generar con el comando de instalaciÃ³n)

### Variables de Airflow

Configurar en la UI de Airflow (Admin â†’ Variables) o vÃ­a CLI:

```bash
# TamaÃ±o de muestra para transacciones (opcional, None = todas)
airflow variables set sales_transactions_sample_size 100000

# ParÃ¡metros FP-Growth
airflow variables set sales_fp_growth_min_support 0.05
airflow variables set sales_fp_growth_min_confidence 0.4
```

### Conexiones de Airflow

La conexiÃ³n `sales_postgres` se crea automÃ¡ticamente durante `airflow-init`. Si necesitas modificarla:

```bash
airflow connections add 'sales_postgres' \
  --conn-uri 'postgresql://sales:sales@postgres:5432/sales'
```

## ğŸš€ Uso

### Acceder a la UI de Airflow

1. Abrir navegador en: http://localhost:8085
2. Credenciales por defecto:
   - Usuario: `admin`
   - ContraseÃ±a: `admin`

### Ejecutar DAGs

1. En la UI, ir a **DAGs**
2. Activar el DAG deseado (toggle ON/OFF)
3. Hacer clic en **Trigger DAG** (â–¶ï¸) para ejecuciÃ³n manual

### Orden recomendado de ejecuciÃ³n

1. **Cargar datos** (ver secciÃ³n [Carga de Datos](#carga-de-datos))
2. `categories_reference_pipeline` - AnÃ¡lisis de categorÃ­as
3. `transactions_quality_pipeline` - AnÃ¡lisis de calidad de transacciones
4. `advanced_sales_analytics` - AnÃ¡lisis avanzados (temporal, clientes, productos, FP-Growth)

### Monitoreo

- **Logs de tareas**: Click en la tarea â†’ Logs
- **Grafana/Spark UI**: http://localhost:8080 (Spark Master)
- **Postgres**: `localhost:5432` (usuario: `airflow` / `sales`)

## ğŸ“ Estructura del Proyecto

```
airflow/
â”œâ”€â”€ dags/                    # DAGs de Airflow
â”‚   â”œâ”€â”€ categories_dag.py
â”‚   â”œâ”€â”€ transactions_dag.py
â”‚   â””â”€â”€ advanced_analysis_dag.py
â”œâ”€â”€ includes/                # MÃ³dulos compartidos
â”‚   â”œâ”€â”€ bootstrap.py         # InicializaciÃ³n de rutas
â”‚   â”œâ”€â”€ pipeline_context.py  # Context manager para pipeline
â”‚   â”œâ”€â”€ tasks.py             # Tareas reutilizables
â”‚   â””â”€â”€ store_service.py     # Servicio de consulta de tiendas
â”œâ”€â”€ config/                  # ConfiguraciÃ³n de Spark
â”‚   â””â”€â”€ spark_config.py
â”œâ”€â”€ src/                     # MÃ³dulos de anÃ¡lisis
â”‚   â”œâ”€â”€ data_loader.py
â”‚   â”œâ”€â”€ pipeline.py
â”‚   â”œâ”€â”€ eda_analyzer.py
â”‚   â”œâ”€â”€ visualizer.py
â”‚   â”œâ”€â”€ utils.py
â”‚   â””â”€â”€ analyzers/
â”‚       â”œâ”€â”€ customer_analyzer.py
â”‚       â”œâ”€â”€ temporal_analyzer.py
â”‚       â””â”€â”€ product_analyzer.py
â”œâ”€â”€ data/                    # Datos CSV (montados en contenedor)
â”‚   â”œâ”€â”€ products/
â”‚   â””â”€â”€ transactions/
â”œâ”€â”€ docker/                  # ConfiguraciÃ³n Docker
â”‚   â”œâ”€â”€ airflow/
â”‚   â”‚   â””â”€â”€ Dockerfile
â”‚   â””â”€â”€ postgres/
â”‚       â””â”€â”€ init-sales-db.sh
â”œâ”€â”€ scripts/                 # Scripts de utilidad
â”‚   â”œâ”€â”€ load_data.sh
â”‚   â””â”€â”€ load_data.bat
â”œâ”€â”€ logs/                    # Logs de Airflow (volumen)
â”œâ”€â”€ plugins/                 # Plugins personalizados
â”œâ”€â”€ docker-compose.yml       # OrquestaciÃ³n de servicios
â”œâ”€â”€ requirements.txt         # Dependencias Python
â”œâ”€â”€ env.template             # Template de variables de entorno
â””â”€â”€ README.md                # Este archivo
```

## ğŸ”„ DAGs Disponibles

### 1. `categories_reference_pipeline`

**DescripciÃ³n**: Analiza datasets de referencia (categorÃ­as y productos-categorÃ­as).

**Tareas**:

- `analyze_categories` â†’ `analyze_product_categories`

**DuraciÃ³n estimada**: 1-2 minutos

### 2. `transactions_quality_pipeline`

**DescripciÃ³n**: Ejecuta anÃ¡lisis de calidad y datasets explodidos de transacciones.

**Tareas**:

- `analyze_transactions` â†’ `analyze_transactions_exploded`

**DuraciÃ³n estimada**: 5-10 minutos (depende del tamaÃ±o de datos)

### 3. `advanced_sales_analytics`

**DescripciÃ³n**: AnÃ¡lisis completo con procesamiento paralelo por tienda y FP-Growth distribuido.

**Tareas**:

- `temporal_analysis` (paralelo)
- `customer_analysis` (paralelo)
- `global_product_analysis` (paralelo)
- `fetch_store_ids` â†’ `analyze_store[store_1, store_2, ...]` (paralelo por tienda)
- `train_fp_growth` (distribuido en Spark cluster)

**Flujo**:

```
[temporal, customers, products]
    â†“
[analyze_store Ã— N tiendas] (paralelo)
    â†“
train_fp_growth
```

**DuraciÃ³n estimada**: 15-30 minutos (depende del nÃºmero de tiendas y tamaÃ±o de datos)

## ğŸ“¥ Carga de Datos

### Requisitos previos

- Servicios de Docker Compose ejecutÃ¡ndose
- Base de datos `sales` creada (se crea automÃ¡ticamente en `airflow-init`)

### OpciÃ³n 1: Script Bash (Linux/Mac)

```bash
# Desde la raÃ­z del proyecto airflow
./scripts/load_data.sh [ruta_a_data]

# Ejemplo con ruta relativa al proyecto principal
./scripts/load_data.sh ../product-sales-analytics/data
```

### OpciÃ³n 2: Script Batch (Windows)

```cmd
REM Desde la raÃ­z del proyecto airflow
scripts\load_data.bat [ruta_a_data]

REM Ejemplo
scripts\load_data.bat ..\product-sales-analytics\data
```

### OpciÃ³n 3: Manual con psql

```bash
# Conectar al contenedor Postgres
docker compose exec postgres psql -U sales -d sales

# Ejecutar comandos COPY manualmente
\copy categories FROM '/path/to/Categories.csv' WITH (FORMAT csv, DELIMITER '|');
```

### Verificar carga

```bash
# Conectar y consultar
docker compose exec postgres psql -U sales -d sales -c "SELECT COUNT(*) FROM transactions;"
```

## ğŸ” Troubleshooting

### Error: "ModuleNotFoundError: No module named 'config'"

**Causa**: Los mÃ³dulos `config/` o `src/` no estÃ¡n disponibles en el contenedor.

**SoluciÃ³n**:

1. Verificar que las carpetas `config/` y `src/` existen en el directorio `airflow/`
2. Verificar que los volÃºmenes estÃ¡n montados correctamente en `docker-compose.yml`
3. Verificar que el volumen estÃ¡ montado correctamente en `docker-compose.yml`

### Error: "Connection refused" a Postgres

**Causa**: Postgres no estÃ¡ listo o las credenciales son incorrectas.

**SoluciÃ³n**:

```bash
# Verificar estado
docker compose ps postgres

# Ver logs
docker compose logs postgres

# Reiniciar servicio
docker compose restart postgres
```

### Error: "Spark master not available"

**Causa**: El clÃºster Spark no estÃ¡ iniciado o la URL es incorrecta.

**SoluciÃ³n**:

```bash
# Verificar servicios Spark
docker compose ps | grep spark

# Ver logs del master
docker compose logs spark-master

# Verificar URL en .env: SPARK_MASTER_URL=spark://spark-master:7077
```

### DAGs no aparecen en la UI

**Causa**: Errores de sintaxis o imports en los DAGs.

**SoluciÃ³n**:

```bash
# Verificar logs del scheduler
docker compose logs airflow-scheduler

# Validar DAGs
docker compose exec airflow-scheduler airflow dags list

# Ver errores especÃ­ficos
docker compose exec airflow-scheduler airflow dags list-import-errors
```

### Permisos en volÃºmenes (Linux/Mac)

**Causa**: Problemas de permisos con el usuario de Airflow.

**SoluciÃ³n**:

```bash
# Ajustar permisos
sudo chown -R 50000:0 airflow/logs airflow/plugins

# O regenerar AIRFLOW_UID
echo -e "AIRFLOW_UID=$(id -u)" > .env
```

## ğŸ”— Enlaces Ãštiles

- **Airflow UI**: http://localhost:8085
- **Spark Master UI**: http://localhost:8080
- **DocumentaciÃ³n Airflow**: https://airflow.apache.org/docs/
- **DocumentaciÃ³n Spark**: https://spark.apache.org/docs/latest/

## ğŸ“ Notas

- Los resultados se guardan en `output/` dentro del proyecto principal (montado como volumen)
- Los anÃ¡lisis por tienda generan CSVs en `output/stores/<store_id>/`
- FP-Growth guarda resultados en `output/data/fp_growth_*`
- El pipeline de recomendaciones escribe JSON consumibles por el frontend en `output/recommendations/{product_recs,customer_recs}.json`
- Los logs de Airflow se almacenan en `airflow/logs/` (persisten entre reinicios)

## ğŸ‘¥ Autores

- Juan David Colonia Aldana - A00395956
- Miguel Ãngel Gonzalez Arango - A00395687

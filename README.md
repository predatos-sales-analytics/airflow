# ðŸ“Š Pipeline de AnÃ¡lisis de Ventas con Apache Spark

Pipelines de procesamiento desarrollados en Apache Spark para analizar el comportamiento de ventas, clientes y productos. El proyecto se ejecuta dentro del directorio `airflow/`, pero toda la orquestaciÃ³n se realiza con scripts ligeros y contenedores Spark + PostgreSQL.

## ðŸ‘¥ Autores

- Juan David Colonia Aldana â€“ A00395956
- Miguel Ãngel Gonzalez Arango â€“ A00395687

## ðŸ§­ Contenido

- [ðŸ“Š Pipeline de AnÃ¡lisis de Ventas con Apache Spark](#-pipeline-de-anÃ¡lisis-de-ventas-con-apache-spark)
  - [ðŸ‘¥ Autores](#-autores)
  - [ðŸ§­ Contenido](#-contenido)
  - [ðŸ—‚ï¸ DescripciÃ³n de los datos](#ï¸-descripciÃ³n-de-los-datos)
  - [ðŸ”¬ MetodologÃ­a de anÃ¡lisis](#-metodologÃ­a-de-anÃ¡lisis)
  - [ðŸ“ˆ Principales hallazgos visuales](#-principales-hallazgos-visuales)
  - [ðŸ§  Resultados de modelos](#-resultados-de-modelos)
  - [ðŸŽ¯ Conclusiones y aplicaciones empresariales](#-conclusiones-y-aplicaciones-empresariales)
  - [ðŸ§µ Pipelines disponibles](#-pipelines-disponibles)
  - [âš™ï¸ EjecuciÃ³n de pipelines](#ï¸-ejecuciÃ³n-de-pipelines)
  - [Estructura del repositorio (carpeta `airflow/`)](#estructura-del-repositorio-carpeta-airflow)

---

## ðŸ—‚ï¸ DescripciÃ³n de los datos

Los datos originales provienen de transacciones minoristas:

- `transactions/`: archivos `*_Tran.csv` con el histÃ³rico de compras. Cada registro trae fecha, tienda, cliente y lista de productos.
- `Categories.csv` y `Product_Categories.csv`: catÃ¡logo de productos y su relaciÃ³n con categorÃ­as.
- Fuente: entregables del curso.

Se cargan en PostgreSQL mediante los scripts `scripts/linux/windows/load_data.*` y luego Spark accede vÃ­a JDBC para todos los pipelines.

---

## ðŸ”¬ MetodologÃ­a de anÃ¡lisis

1. **Ingesta**: Spark lee las tablas principales (`transactions`, `categories`, `product_categories`) directamente desde PostgreSQL.
2. **Enriquecimiento**: se explotan las listas de productos, se calculan mÃ©tricas por cliente, categorÃ­a y fecha, y se estandarizan fechas y tipos numÃ©ricos.
3. **AnÃ¡lisis ejecutivo**: agregaciones en Spark SQL para mÃ©tricas de negocio (transacciones, productos, top-N).
4. **AnalÃ­tica temporal**: series de tiempo diarias, semanales y mensuales; patrones por dÃ­a de semana; boxplots por categorÃ­a.
5. **Modelos avanzados**:
   - **Clustering**: K-Means con variables de frecuencia, volumen y diversidad de compra.
   - **Recomendaciones**: FP-Growth para reglas de asociaciÃ³n. Se generan salidas por producto y por cliente.
6. **ExportaciÃ³n**: todos los resultados se escriben como JSON en `output/` para ser consumidos por el frontend.

---

## ðŸ“ˆ Principales hallazgos visuales

- **Patrones temporales**: se observan picos los fines de semana y un comportamiento mensual con ligeras estacionalidades.
- **Boxplots por categorÃ­a**: algunas familias (por ejemplo, â€œFrutas y verdurasâ€) concentran la mayor parte de unidades, mientras categorÃ­as especializadas tienen variaciones menores.
- **Heatmap**: la correlaciÃ³n revela que frecuencia de compra y diversidad de productos estÃ¡n positivamente relacionadas con el volumen total vendido.

Los archivos JSON en `output/analytics/` contienen las series y distribuciones que alimentan las grÃ¡ficas del dashboard React.

---

## ðŸ§  Resultados de modelos

- **SegmentaciÃ³n (K-Means)**  
  Se generan cuatro clusters con perfiles claros:

  1. **VIP/Premium**: alta frecuencia y volumen; candidatos a programas de fidelizaciÃ³n robustos.
  2. **Exploradores**: compran gran variedad de productos/categorÃ­as; responden bien a lanzamientos.
  3. **Ocasionales**: pocas compras al aÃ±o; requieren campaÃ±as de reactivaciÃ³n.
  4. **Clientes nuevos**: transacciones recientes y de bajo volumen; conviene guiarlos hacia categorÃ­as rentables.

- **Recomendaciones (FP-Growth)**
  - **Producto â†’ producto**: reglas con lift > 3 identifican complementos naturales (ej. categorÃ­as frescas + abarrotes).
  - **Cliente â†’ producto**: sugerencias personalizadas derivadas de las reglas y del historial individual.
  - Se guardan estadÃ­sticas del dataset para evitar recalcular FP-Growth si no cambian los datos.

---

## ðŸŽ¯ Conclusiones y aplicaciones empresariales

- El pipeline permite monitorear el negocio a nivel ejecutivo, detectar patrones temporales y segmentar clientes sin depender de herramientas externas.
- Las reglas de asociaciÃ³n alimentan estrategias de cross-selling tanto en tienda como en canales digitales.
- Los clusters facilitan campaÃ±as especÃ­ficas: retenciÃ³n de VIPs, incentivos a exploradores, reactivaciÃ³n de clientes ocasionales.
- Exportar en JSON permite integrar fÃ¡cilmente un dashboard React o cualquier otra aplicaciÃ³n que consuma APIs o archivos estÃ¡ticos.

---

## ðŸ§µ Pipelines disponibles

| Pipeline            | Objetivo                                                                                                                                  | Salidas principales                                  |
| ------------------- | ----------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------- |
| `executive_summary` | KPIs ejecutivos: totales de transacciones y ventas, top 10 de productos, clientes y categorÃ­as, dÃ­as pico.                                | `output/summary/basic_metrics.json`, `top_10_*.json` |
| `analytics`         | Series de tiempo (diaria/semanal/mensual), patrones por dÃ­a de la semana, boxplots por categorÃ­a y heatmap de correlaciones.              | `output/analytics/*.json`                            |
| `clustering`        | SegmentaciÃ³n K-Means en cuatro perfiles (VIP, exploradores, ocasionales, nuevos) con mÃ©tricas y recomendaciones por cluster.              | `output/advanced/clustering/*.json`                  |
| `recommendations`   | Reglas de asociaciÃ³n FP-Growth + sugerencias por producto y por cliente. Reutiliza resultados si las estadÃ­sticas del dataset no cambian. | `output/advanced/recommendations/*.json`             |

---

## âš™ï¸ EjecuciÃ³n de pipelines

1. **Levantar servicios** (Spark master/worker, PostgreSQL, cliente):

   ```bash
   docker compose up -d
   ```

2. **Cargar datos** (si es la primera vez):

   ```bash
   ./scripts/linux/load_data.sh ../data
   # o en Windows
   scripts/windows/load_data.bat ..\data
   ```

3. **Ejecutar pipelines** dentro del contenedor `spark-client`:

   ```bash
   docker compose exec spark-client python src/run_pipeline.py executive_summary
   docker compose exec spark-client python src/run_pipeline.py analytics
   docker compose exec spark-client python src/run_pipeline.py clustering --n-clusters 4
   docker compose exec spark-client python src/run_pipeline.py recommendations
   ```

4. **Reutilizar FP-Growth**: si los stats de canastas no cambian, el pipeline de recomendaciones reutiliza los resultados previos (`output/data/fp_growth_*`).

5. **Copiar salidas al frontend**: mover `output/summary`, `output/analytics`, `output/advanced/*`, `output/advanced/recommendations` a `sales-frontend/public/data/`.

---

## Estructura del repositorio (carpeta `airflow/`)

```
airflow/
â”œâ”€â”€ docker-compose.yml          # Servicios Spark + PostgreSQL + cliente
â”œâ”€â”€ requirements.txt            # Dependencias (Spark, pandas, sklearn, etc.)
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ run_pipeline.py         # CLI para ejecutar pipelines Spark
â”‚   â”œâ”€â”€ config/spark_config.py  # ConfiguraciÃ³n del SparkSession
â”‚   â”œâ”€â”€ pipelines/              # Pipelines: executive, analytics, clustering, recommendations
â”‚   â”œâ”€â”€ analyzers/              # LÃ³gica de mÃ©tricas, estadÃ­stica y modelos
â”‚   â”œâ”€â”€ data_loader.py          # Lectura JDBC desde PostgreSQL
â”‚   â””â”€â”€ json_exporter.py        # Utilidades para escribir JSON
â”œâ”€â”€ scripts/                    # Scripts para cargar datos y ejecutar pipelines (Linux/Windows)
â”œâ”€â”€ data/                       # CSV originales (montar localmente)
â””â”€â”€ output/                     # Resultados JSON para el frontend
```

> **Nota**: Esta carpeta se llama `airflow` por el contexto original, pero hoy los pipelines se ejecutan directamente sobre Spark con scripts Dockerizados. No se requiere un scheduler externo.
